# Apple Vision OCR æŠ€æœ¯æ–‡æ¡£

## 1. æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» Apple Vision æ¡†æ¶çš„æ–‡æœ¬è¯†åˆ«ï¼ˆOCRï¼‰åŠŸèƒ½ï¼ŒåŒ…æ‹¬ API ç»“æ„ã€è¿”å›æ•°æ®æ ¼å¼ã€bounding box ä¿¡æ¯ç­‰æŠ€æœ¯ç»†èŠ‚ï¼Œä»¥å¸®åŠ©è¯„ä¼°å…¶æ˜¯å¦æ»¡è¶³ SyncNos çš„èŠå¤©æˆªå›¾ OCR éœ€æ±‚ã€‚

## 2. Vision æ¡†æ¶ç®€ä»‹

Vision æ˜¯ Apple æä¾›çš„åŸç”Ÿè®¡ç®—æœºè§†è§‰æ¡†æ¶ï¼Œä» macOS 10.13 / iOS 11 å¼€å§‹æ”¯æŒã€‚æ–‡æœ¬è¯†åˆ«åŠŸèƒ½åœ¨ macOS 10.15 / iOS 13 ä¸­å¼•å…¥ï¼Œç»è¿‡å¤šæ¬¡è¿­ä»£å·²ç›¸å½“æˆç†Ÿã€‚

### 2.1 å¹³å°æ”¯æŒ

| å¹³å° | æœ€ä½ç‰ˆæœ¬ | æ¨èç‰ˆæœ¬ |
|-----|---------|---------|
| macOS | 10.15+ | 14.0+ï¼ˆSonomaï¼‰|
| iOS | 13.0+ | 17.0+ |
| iPadOS | 13.0+ | 17.0+ |
| visionOS | 1.0+ | 2.0+ |

### 2.2 SyncNos å…¼å®¹æ€§

SyncNos ç›®æ ‡å¹³å°ä¸º **macOS 14.0+**ï¼Œå®Œå…¨æ”¯æŒ Vision æ¡†æ¶çš„æ‰€æœ‰ OCR åŠŸèƒ½ï¼ŒåŒ…æ‹¬æœ€æ–°çš„ Swift APIï¼ˆ`RecognizeTextRequest`ï¼‰ã€‚

---

## 3. API æ¶æ„

### 3.1 ä¸¤ç§ API é£æ ¼

Vision æ¡†æ¶æä¾›ä¸¤ç§ API é£æ ¼ï¼š

#### ä¼ ç»Ÿ Objective-C é£æ ¼ï¼ˆmacOS 10.15+ï¼‰

```swift
// ä½¿ç”¨ VNRecognizeTextRequest
let request = VNRecognizeTextRequest { request, error in
    guard let observations = request.results as? [VNRecognizedTextObservation] else { return }
    // å¤„ç†ç»“æœ
}
```

#### ç°ä»£ Swift é£æ ¼ï¼ˆmacOS 15.0+ / iOS 18.0+ï¼‰

```swift
// ä½¿ç”¨ RecognizeTextRequestï¼ˆSwift åŸç”Ÿï¼‰
let request = RecognizeTextRequest()
let results = try await request.perform(on: cgImage)
```

**å»ºè®®**ï¼šSyncNos ç›®æ ‡ä¸º macOS 14.0+ï¼Œåº”ä½¿ç”¨ **ä¼ ç»Ÿ Objective-C é£æ ¼ API**ï¼ˆ`VNRecognizeTextRequest`ï¼‰ä»¥ä¿æŒå…¼å®¹æ€§ã€‚

### 3.2 è¯†åˆ«æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CGImage   â”‚ â”€â”€â–º â”‚ VNImageRequestHandler â”‚ â”€â”€â–º â”‚ VNRecognizeTextRequest â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                                          â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚ [VNRecognizedText-   â”‚
                                               â”‚  Observation]        â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                                          â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚ - boundingBox: CGRect â”‚
                                               â”‚ - topCandidates(N)    â”‚
                                               â”‚ - confidence: Float   â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. è¿”å›æ•°æ®ç»“æ„è¯¦è§£

### 4.1 VNRecognizedTextObservation

æ¯ä¸ªè¯†åˆ«åˆ°çš„æ–‡æœ¬åŒºåŸŸéƒ½ä¼šè¿”å›ä¸€ä¸ª `VNRecognizedTextObservation` å¯¹è±¡ã€‚

#### 4.1.1 ç»§æ‰¿å±‚çº§

```
VNObservation
    â””â”€â”€ VNDetectedObjectObservation
            â””â”€â”€ VNRectangleObservation
                    â””â”€â”€ VNRecognizedTextObservation
```

#### 4.1.2 ä¸»è¦å±æ€§

| å±æ€§ | ç±»å‹ | æè¿° |
|-----|------|------|
| `boundingBox` | `CGRect` | æ–‡æœ¬åŒºåŸŸçš„è¾¹ç•Œæ¡†ï¼ˆ**å½’ä¸€åŒ–åæ ‡ 0~1**ï¼‰ |
| `topLeft` | `CGPoint` | å·¦ä¸Šè§’åæ ‡ï¼ˆå½’ä¸€åŒ–ï¼‰ |
| `topRight` | `CGPoint` | å³ä¸Šè§’åæ ‡ï¼ˆå½’ä¸€åŒ–ï¼‰ |
| `bottomLeft` | `CGPoint` | å·¦ä¸‹è§’åæ ‡ï¼ˆå½’ä¸€åŒ–ï¼‰ |
| `bottomRight` | `CGPoint` | å³ä¸‹è§’åæ ‡ï¼ˆå½’ä¸€åŒ–ï¼‰ |
| `confidence` | `VNConfidence` | æ£€æµ‹ç½®ä¿¡åº¦ï¼ˆ0~1ï¼‰ |
| `uuid` | `UUID` | å”¯ä¸€æ ‡è¯†ç¬¦ |

#### 4.1.3 æ–¹æ³•

| æ–¹æ³• | è¿”å›ç±»å‹ | æè¿° |
|-----|---------|------|
| `topCandidates(_ maxCandidates: Int)` | `[VNRecognizedText]` | è¿”å›æ’åé å‰çš„è¯†åˆ«å€™é€‰ |

### 4.2 VNRecognizedText

æ¯ä¸ªè¯†åˆ«å€™é€‰åŒ…å«å…·ä½“çš„æ–‡æœ¬å†…å®¹ã€‚

#### 4.2.1 å±æ€§

| å±æ€§ | ç±»å‹ | æè¿° |
|-----|------|------|
| `string` | `String` | è¯†åˆ«å‡ºçš„æ–‡æœ¬å†…å®¹ |
| `confidence` | `VNConfidence` | è¯†åˆ«ç½®ä¿¡åº¦ï¼ˆ0~1ï¼‰ |

#### 4.2.2 æ–¹æ³•

| æ–¹æ³• | è¿”å›ç±»å‹ | æè¿° |
|-----|---------|------|
| `boundingBox(for: Range<String.Index>)` | `VNRectangleObservation?` | è·å–æ–‡æœ¬å­ä¸²çš„è¾¹ç•Œæ¡† |

### 4.3 åæ ‡ç³»ç»Ÿ

**é‡è¦**ï¼šVision æ¡†æ¶ä½¿ç”¨ **å½’ä¸€åŒ–åæ ‡ç³»**ï¼Œä¸ UIKit/AppKit åæ ‡ç³»ä¸åŒï¼š

```
Vision åæ ‡ç³»:                 AppKit/UIKit åæ ‡ç³»:
(0,1) â”€â”€â”€â”€â”€â”€â”€ (1,1)           (0,0) â”€â”€â”€â”€â”€â”€â”€ (w,0)
  â”‚             â”‚               â”‚             â”‚
  â”‚             â”‚               â”‚             â”‚
(0,0) â”€â”€â”€â”€â”€â”€â”€ (1,0)           (0,h) â”€â”€â”€â”€â”€â”€â”€ (w,h)
  åŸç‚¹åœ¨å·¦ä¸‹è§’                   åŸç‚¹åœ¨å·¦ä¸Šè§’
```

#### 4.3.1 åæ ‡è½¬æ¢

> âš ï¸ **é‡è¦è­¦å‘Š**ï¼š`VNImageRectForNormalizedRect` **ä¸ä¼šç¿»è½¬ Y è½´**ï¼
> 
> è¯¥å‡½æ•°åªåšç®€å•çš„ç¼©æ”¾ï¼Œè¿”å›çš„åæ ‡ä»ç„¶æ˜¯åŸç‚¹åœ¨å·¦ä¸‹è§’çš„åæ ‡ç³»ã€‚
> å¦‚æœéœ€è¦ä¸å›¾åƒåæ ‡ç³»ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼‰åŒ¹é…ï¼Œå¿…é¡»æ‰‹åŠ¨ç¿»è½¬ Y è½´ã€‚

**æ­£ç¡®çš„æ‰‹åŠ¨è½¬æ¢æ–¹å¼ï¼š**

```swift
/// å°† Vision å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºå›¾åƒåƒç´ åæ ‡ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼‰
func convertToImageCoordinates(
    _ normalizedBox: CGRect,
    imageSize: CGSize
) -> CGRect {
    // Vision åæ ‡ç³»åŸç‚¹åœ¨å·¦ä¸‹è§’ï¼Œéœ€è¦æ‰‹åŠ¨ç¿»è½¬ Y è½´
    let x = normalizedBox.origin.x * imageSize.width
    let y = imageSize.height * (1 - normalizedBox.origin.y - normalizedBox.height)
    let width = normalizedBox.width * imageSize.width
    let height = normalizedBox.height * imageSize.height
    
    return CGRect(x: x, y: y, width: width, height: height)
}
```

**VNImageRectForNormalizedRect çš„å®é™…è¡Œä¸ºï¼ˆä»…ç¼©æ”¾ï¼Œä¸ç¿»è½¬ï¼‰ï¼š**

```swift
// âš ï¸ æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸ç¿»è½¬ Y è½´ï¼
let pixelRect = VNImageRectForNormalizedRect(boundingBox, Int(width), Int(height))
// ç»“æœï¼špixelRect.origin.y = boundingBox.origin.y * height
// è¿™æ„å‘³ç€ Y å€¼è¶Šå¤§ = è·ç¦»å›¾åƒåº•éƒ¨è¶Šè¿‘ï¼ˆä¸æ ‡å‡†å›¾åƒåæ ‡ç³»ç›¸åï¼‰
```

**å…³é”®åŒºåˆ«ï¼š**

| åæ ‡ç³» | Y å€¼å«ä¹‰ | åŸç‚¹ä½ç½® |
|-------|---------|---------|
| Vision å½’ä¸€åŒ–åæ ‡ | Y=0 åœ¨åº•éƒ¨ï¼ŒY=1 åœ¨é¡¶éƒ¨ | å·¦ä¸‹è§’ |
| VNImageRectForNormalizedRect è¾“å‡º | Y=0 åœ¨åº•éƒ¨ï¼ŒY=height åœ¨é¡¶éƒ¨ | å·¦ä¸‹è§’ |
| æ ‡å‡†å›¾åƒåæ ‡ï¼ˆUIKit/AppKitï¼‰ | Y=0 åœ¨é¡¶éƒ¨ï¼ŒY=height åœ¨åº•éƒ¨ | å·¦ä¸Šè§’ |

**åœ¨ SyncNos ä¸­çš„å®ç°ï¼š**

`VisionOCRService.swift` ä½¿ç”¨æ‰‹åŠ¨åæ ‡è½¬æ¢ä»¥ä¿è¯ä¸ `ChatOCRParser` çš„æ’åºé€»è¾‘å…¼å®¹ï¼š

```swift
let x = normalizedBox.origin.x * imageSize.width
let y = imageSize.height * (1 - normalizedBox.origin.y - normalizedBox.height)
let width = normalizedBox.width * imageSize.width
let height = normalizedBox.height * imageSize.height
let pixelRect = CGRect(x: x, y: y, width: width, height: height)
```

---

## 5. å®Œæ•´ç¤ºä¾‹ä»£ç 

### 5.1 åŸºç¡€å®ç°

```swift
import Vision
import AppKit

/// Vision OCR æœåŠ¡ï¼ˆéµå¾ª SyncNos çš„ OCRAPIServiceProtocolï¼‰
final class VisionOCRService: OCRAPIServiceProtocol {
    
    private let logger: LoggerServiceProtocol
    
    init(logger: LoggerServiceProtocol = DIContainer.shared.loggerService) {
        self.logger = logger
    }
    
    // MARK: - OCRAPIServiceProtocol
    
    func recognize(_ image: NSImage) async throws -> OCRResult {
        let (result, _, _) = try await recognizeWithRaw(image, config: .default)
        return result
    }
    
    func recognizeWithRaw(
        _ image: NSImage,
        config: OCRRequestConfig
    ) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        guard let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
            throw OCRServiceError.invalidImage
        }
        
        let imageSize = CGSize(
            width: CGFloat(cgImage.width),
            height: CGFloat(cgImage.height)
        )
        
        logger.info("[VisionOCR] Starting recognition, image size: \(imageSize)")
        
        // åˆ›å»ºæ–‡æœ¬è¯†åˆ«è¯·æ±‚
        let request = VNRecognizeTextRequest()
        request.recognitionLevel = .accurate
        request.usesLanguageCorrection = true
        request.recognitionLanguages = ["zh-Hans", "zh-Hant", "en-US"]
        request.revision = VNRecognizeTextRequestRevision3
        
        // æ‰§è¡Œè¯·æ±‚
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        return try await withCheckedThrowingContinuation { continuation in
            DispatchQueue.global(qos: .userInitiated).async {
                do {
                    try handler.perform([request])
                    
                    guard let observations = request.results else {
                        continuation.resume(throwing: OCRServiceError.noResult)
                        return
                    }
                    
                    let blocks = self.convertToOCRBlocks(
                        observations: observations,
                        imageSize: imageSize
                    )
                    
                    let result = OCRResult(
                        rawText: blocks.map(\.text).joined(separator: "\n"),
                        markdownText: nil,
                        blocks: blocks,
                        processedAt: Date(),
                        coordinateSize: imageSize
                    )
                    
                    // æ„é€  raw responseï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    let rawDict = self.observationsToDict(observations, imageSize: imageSize)
                    let rawData = try? JSONSerialization.data(withJSONObject: rawDict)
                    
                    self.logger.info("[VisionOCR] Recognition completed: \(blocks.count) blocks")
                    
                    continuation.resume(returning: (
                        result: result,
                        rawResponse: rawData ?? Data(),
                        requestJSON: Data()
                    ))
                    
                } catch {
                    self.logger.error("[VisionOCR] Recognition failed: \(error)")
                    continuation.resume(throwing: error)
                }
            }
        }
    }
    
    func testConnection() async throws -> Bool {
        // Vision æ¡†æ¶æ— éœ€è¿æ¥æµ‹è¯•ï¼Œå§‹ç»ˆå¯ç”¨
        return true
    }
    
    // MARK: - Private Methods
    
    private func convertToOCRBlocks(
        observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [OCRBlock] {
        return observations.compactMap { observation in
            guard let topCandidate = observation.topCandidates(1).first else {
                return nil
            }
            
            // è½¬æ¢åæ ‡
            let pixelRect = VNImageRectForNormalizedRect(
                observation.boundingBox,
                Int(imageSize.width),
                Int(imageSize.height)
            )
            
            return OCRBlock(
                text: topCandidate.string,
                label: "text",
                bbox: pixelRect
            )
        }
    }
    
    private func observationsToDict(
        _ observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [[String: Any]] {
        return observations.compactMap { obs in
            guard let text = obs.topCandidates(1).first else { return nil }
            
            let pixelRect = VNImageRectForNormalizedRect(
                obs.boundingBox,
                Int(imageSize.width),
                Int(imageSize.height)
            )
            
            return [
                "text": text.string,
                "confidence": text.confidence,
                "boundingBox": [
                    "x": pixelRect.origin.x,
                    "y": pixelRect.origin.y,
                    "width": pixelRect.width,
                    "height": pixelRect.height
                ],
                "normalizedBoundingBox": [
                    "x": obs.boundingBox.origin.x,
                    "y": obs.boundingBox.origin.y,
                    "width": obs.boundingBox.width,
                    "height": obs.boundingBox.height
                ]
            ]
        }
    }
}
```

### 5.2 è·å–å­—ç¬¦çº§è¾¹ç•Œæ¡†

```swift
/// è·å–å•ä¸ªå­—ç¬¦çš„è¾¹ç•Œæ¡†
func getCharacterBoundingBoxes(
    observation: VNRecognizedTextObservation,
    imageSize: CGSize
) -> [(character: Character, bbox: CGRect)] {
    guard let recognizedText = observation.topCandidates(1).first else {
        return []
    }
    
    let string = recognizedText.string
    var results: [(Character, CGRect)] = []
    
    for (index, character) in string.enumerated() {
        let startIndex = string.index(string.startIndex, offsetBy: index)
        let endIndex = string.index(startIndex, offsetBy: 1)
        let range = startIndex..<endIndex
        
        if let charObservation = try? recognizedText.boundingBox(for: range) {
            let pixelRect = VNImageRectForNormalizedRect(
                charObservation.boundingBox,
                Int(imageSize.width),
                Int(imageSize.height)
            )
            results.append((character, pixelRect))
        }
    }
    
    return results
}
```

### 5.3 å®æ—¶ç›¸æœºè¯†åˆ«

```swift
import AVFoundation

/// å¤„ç†ç›¸æœºå¸§çš„ OCR
func processVideoFrame(_ sampleBuffer: CMSampleBuffer) {
    guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
    
    let request = VNRecognizeTextRequest { request, error in
        // å¤„ç†ç»“æœ
    }
    request.recognitionLevel = .fast  // å®æ—¶åœºæ™¯ä½¿ç”¨ fast æ¨¡å¼
    
    let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
    try? handler.perform([request])
}
```

---

## 6. é…ç½®é€‰é¡¹è¯¦è§£

### 6.1 è¯†åˆ«çº§åˆ« (recognitionLevel)

```swift
request.recognitionLevel = .accurate  // æˆ– .fast
```

| çº§åˆ« | é€Ÿåº¦ | å‡†ç¡®åº¦ | é€‚ç”¨åœºæ™¯ |
|-----|------|--------|---------|
| `.fast` | â­â­â­â­â­ | â­â­â­ | å®æ—¶ç›¸æœºã€è§†é¢‘æµ |
| `.accurate` | â­â­â­ | â­â­â­â­â­ | é™æ€å›¾ç‰‡ã€é«˜ç²¾åº¦éœ€æ±‚ |

**å»ºè®®**ï¼šSyncNos å¤„ç†é™æ€æˆªå›¾ï¼Œåº”ä½¿ç”¨ `.accurate`ã€‚

### 6.2 è¯­è¨€æ ¡æ­£ (usesLanguageCorrection)

```swift
request.usesLanguageCorrection = true
```

- `true`ï¼šå¯ç”¨åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¡æ­£ï¼Œå‡å°‘è¯¯è¯†åˆ«
- `false`ï¼šç¦ç”¨æ ¡æ­£ï¼Œé€‚åˆéæ ‡å‡†æ–‡æœ¬ï¼ˆä»£ç ã€ç‰¹æ®Šç¬¦å·ï¼‰

### 6.3 è‡ªåŠ¨è¯­è¨€æ£€æµ‹ (automaticallyDetectsLanguage)

```swift
// macOS 13+ / iOS 16+ æ”¯æŒè‡ªåŠ¨è¯­è¨€æ£€æµ‹
request.automaticallyDetectsLanguage = true
```

å¯ç”¨åï¼ŒVision ä¼šè‡ªåŠ¨æ£€æµ‹å›¾ç‰‡ä¸­çš„è¯­è¨€ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šã€‚**SyncNos é»˜è®¤å¯ç”¨æ­¤åŠŸèƒ½ã€‚**

### 6.4 è¯†åˆ«è¯­è¨€ (recognitionLanguages)

```swift
// ä½œä¸ºè‡ªåŠ¨æ£€æµ‹çš„ä¼˜å…ˆçº§æç¤ºå’Œ fallback
request.recognitionLanguages = ["zh-Hans", "zh-Hant", "en-US"]
```

**æ”¯æŒçš„è¯­è¨€**ï¼ˆmacOS 14 / iOS 17ï¼ŒAccurate æ¨¡å¼ï¼Œå…± 30 ç§ï¼‰ï¼š

> ä»¥ä¸‹åˆ—è¡¨æ¥è‡ª `VNRecognizeTextRequest.supportedRecognitionLanguages()` è¿è¡Œæ—¶æŸ¥è¯¢ç»“æœ

| è¯­è¨€åˆ†ç±» | è¯­è¨€ | ä»£ç  |
|---------|-----|------|
| **ä¸œäºšè¯­è¨€** | ç®€ä½“ä¸­æ–‡ | `zh-Hans` |
|  | ç¹ä½“ä¸­æ–‡ | `zh-Hant` |
|  | ç²¤è¯­ï¼ˆç®€ä½“ï¼‰ | `yue-Hans` |
|  | ç²¤è¯­ï¼ˆç¹ä½“ï¼‰ | `yue-Hant` |
|  | æ—¥è¯­ | `ja-JP` |
|  | éŸ©è¯­ | `ko-KR` |
| **è¥¿æ¬§è¯­è¨€** | è‹±è¯­ | `en-US` |
|  | æ³•è¯­ | `fr-FR` |
|  | å¾·è¯­ | `de-DE` |
|  | è¥¿ç­ç‰™è¯­ | `es-ES` |
|  | æ„å¤§åˆ©è¯­ | `it-IT` |
|  | è‘¡è„ç‰™è¯­ï¼ˆå·´è¥¿ï¼‰ | `pt-BR` |
|  | è·å…°è¯­ | `nl-NL` |
| **ä¸œæ¬§è¯­è¨€** | ä¿„è¯­ | `ru-RU` |
|  | ä¹Œå…‹å…°è¯­ | `uk-UA` |
|  | æ³¢å…°è¯­ | `pl-PL` |
|  | æ·å…‹è¯­ | `cs-CZ` |
|  | ç½—é©¬å°¼äºšè¯­ | `ro-RO` |
| **åŒ—æ¬§è¯­è¨€** | ç‘å…¸è¯­ | `sv-SE` |
|  | ä¸¹éº¦è¯­ | `da-DK` |
|  | æŒªå¨è¯­ | `no-NO` |
|  | ä¹¦é¢æŒªå¨è¯­ | `nb-NO` |
|  | æ–°æŒªå¨è¯­ | `nn-NO` |
| **ä¸œå—äºšè¯­è¨€** | æ³°è¯­ | `th-TH` |
|  | è¶Šå—è¯­ | `vi-VT` |
|  | å°å°¼è¯­ | `id-ID` |
|  | é©¬æ¥è¯­ | `ms-MY` |
| **ä¸­ä¸œè¯­è¨€** | é˜¿æ‹‰ä¼¯è¯­ | `ar-SA` |
|  | é˜¿æ‹‰ä¼¯è¯­ï¼ˆçº³å‰è¿ªï¼‰ | `ars-SA` |
|  | åœŸè€³å…¶è¯­ | `tr-TR` |

**Fast æ¨¡å¼åªæ”¯æŒ 6 ç§è¯­è¨€**ï¼š`en-US`, `fr-FR`, `it-IT`, `de-DE`, `es-ES`, `pt-BR`

**æŸ¥è¯¢æ”¯æŒçš„è¯­è¨€**ï¼š

```swift
let supportedLanguages = try? VNRecognizeTextRequest.supportedRecognitionLanguages(
    for: .accurate,
    revision: VNRecognizeTextRequestRevision3
)
print(supportedLanguages ?? [])
// è¾“å‡º: ["en-US", "fr-FR", "it-IT", "de-DE", "es-ES", "pt-BR", "zh-Hans", "zh-Hant", "yue-Hans", "yue-Hant", "ko-KR", "ja-JP", ...]
```

**æ³¨æ„äº‹é¡¹**ï¼š
- âš ï¸ **ä¸­æ–‡ä¸æ—¥è¯­ä¸èƒ½æ··åˆä½¿ç”¨**ï¼šå¦‚æœéœ€è¦åŒæ—¶è¯†åˆ«ä¸­æ—¥æ–‡å†…å®¹ï¼Œéœ€è¦åˆ†ä¸¤æ¬¡è¯·æ±‚
- ä¸­æ–‡ï¼ˆç®€ä½“/ç¹ä½“ï¼‰å¯ä»¥ä¸è‹±è¯­æ··åˆä½¿ç”¨
- å¯ç”¨ `automaticallyDetectsLanguage` åï¼Œç³»ç»Ÿä¼šæ™ºèƒ½é€‰æ‹©æœ€ä½³è¯­è¨€æ¨¡å‹

### 6.5 SyncNos è¯­è¨€é…ç½®åŠŸèƒ½

SyncNos åœ¨ Settings â†’ OCR Settings ä¸­æä¾›è¯­è¨€é…ç½®åŠŸèƒ½ï¼š

#### è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰

å½“ç”¨æˆ·æœªé€‰æ‹©ä»»ä½•è¯­è¨€æ—¶ï¼ŒVision æ¡†æ¶è‡ªåŠ¨æ£€æµ‹å›¾åƒä¸­çš„è¯­è¨€ï¼Œä½¿ç”¨é»˜è®¤ä¼˜å…ˆè¯­è¨€ï¼ˆä¸­æ–‡ç®€ä½“ã€ç¹ä½“ã€è‹±æ–‡ï¼‰ä½œä¸ºæç¤ºã€‚

```swift
// selectedLanguageCodes ä¸ºç©ºæ—¶å¯ç”¨è‡ªåŠ¨æ£€æµ‹
request.automaticallyDetectsLanguage = true
request.recognitionLanguages = ["zh-Hans", "zh-Hant", "en-US"]
```

#### æ‰‹åŠ¨é€‰æ‹©è¯­è¨€

ç”¨æˆ·å¯ä»¥åœ¨ Settings â†’ Chats â†’ OCR Languages ä¸­é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼Œé€‚ç”¨äºç‰¹å®šè¯­è¨€åœºæ™¯ï¼ˆå¦‚æ—¥è¯­ã€éŸ©è¯­ç­‰ï¼‰ã€‚

```swift
// selectedLanguageCodes éç©ºæ—¶ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€
request.automaticallyDetectsLanguage = false
request.recognitionLanguages = configStore.selectedLanguageCodes  // ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€
```

#### ç›¸å…³ä»£ç æ–‡ä»¶

| æ–‡ä»¶ | æè¿° |
|-----|------|
| `OCRConfigStore.swift` | è¯­è¨€é…ç½®å­˜å‚¨ï¼ˆ`OCRLanguage`ï¼Œ30 ç§è¯­è¨€ï¼‰ |
| `VisionOCRService.swift` | æ ¹æ® `OCRConfigStore` åŠ¨æ€è®¾ç½®è¯­è¨€å‚æ•°ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿— |
| `OCRSettingsView.swift` | ç®€æ´çš„è¯­è¨€é€‰æ‹© UIï¼ˆè¯­è¨€é€‰æ‹© Sheet + Debug æµ‹è¯•ï¼‰ |

#### OCR è¯†åˆ«æ—¥å¿—

`VisionOCRService` ä¼šè¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼Œå¸®åŠ©è°ƒè¯•å’ŒéªŒè¯è¯†åˆ«ç»“æœï¼š

```
[VisionOCR] Starting recognition, image size: 1080x1920
[VisionOCR] Language config: Auto (using defaults: zh-Hans, zh-Hant, en-US)
[VisionOCR] âœ… Recognition completed: 25 blocks (from 25 observations)
[VisionOCR] ğŸ“Š Confidence: avg=0.95, min=0.82, max=0.99
[VisionOCR] ğŸŒ Detected scripts: CJK (Chinese/Japanese Kanji), Latin (English/European)
[VisionOCR] ğŸ“ First 5 blocks:
[VisionOCR]   [1] "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸå¥½" (conf: 0.98)
[VisionOCR]   [2] "æ˜¯å•Šï¼Œé€‚åˆå‡ºå»èµ°èµ°" (conf: 0.95)
...
```

æ—¥å¿—å†…å®¹åŒ…æ‹¬ï¼š
- **è¯­è¨€é…ç½®**ï¼šè‡ªåŠ¨æ£€æµ‹ï¼ˆAutoï¼‰æˆ–ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€åˆ—è¡¨
- **ä½¿ç”¨çš„è¯­è¨€åˆ—è¡¨**ï¼šå®é™…ä¼ é€’ç»™ Vision çš„è¯­è¨€ä»£ç 
- **è¯†åˆ«ç»Ÿè®¡**ï¼šå—æ•°é‡ã€ç½®ä¿¡åº¦åˆ†å¸ƒï¼ˆå¹³å‡/æœ€å°/æœ€å¤§ï¼‰
- **æ£€æµ‹åˆ°çš„ä¹¦å†™ç³»ç»Ÿ**ï¼šCJKã€Hiraganaã€Katakanaã€Hangulã€Arabicã€Cyrillicã€Thaiã€Latin
- **å‰ 5 ä¸ªè¯†åˆ«ç»“æœé¢„è§ˆ**ï¼šæ–‡æœ¬å†…å®¹å’Œç½®ä¿¡åº¦

#### Debug æµ‹è¯•åŠŸèƒ½

åœ¨ Settings â†’ OCR Settings ä¸­æä¾› Debug æµ‹è¯•åŠŸèƒ½ï¼ˆ"Test OCR Recognition"ï¼‰ï¼Œæ”¯æŒï¼š

1. **å¯¼å…¥å›¾ç‰‡**ï¼šç‚¹å‡»æŒ‰é’®é€‰æ‹©å›¾ç‰‡ï¼Œæˆ–æ‹–æ”¾å›¾ç‰‡åˆ°çª—å£
2. **å®æ—¶è¯†åˆ«**ï¼šå¯¼å…¥åè‡ªåŠ¨æ‰§è¡Œ OCR è¯†åˆ«
3. **ç»“æœå±•ç¤º**ï¼š
   - ç»Ÿè®¡ä¿¡æ¯ï¼šå—æ•°é‡ã€å¤„ç†æ—¶é—´ã€è¯­è¨€æ¨¡å¼ã€æ£€æµ‹åˆ°çš„ä¹¦å†™ç³»ç»Ÿ
   - è¯†åˆ«æ–‡æœ¬ï¼šå®Œæ•´çš„è¯†åˆ«æ–‡æœ¬å†…å®¹
   - å—è¯¦æƒ…ï¼šæ¯ä¸ªè¯†åˆ«å—çš„æ–‡æœ¬å’Œ bbox åæ ‡

### 6.5 è‡ªå®šä¹‰è¯æ±‡ (customWords)

```swift
request.customWords = ["å¾®ä¿¡", "WeChat", "SyncNos"]
```

æ·»åŠ é¢†åŸŸç‰¹å®šè¯æ±‡ï¼Œæé«˜è¯†åˆ«å‡†ç¡®ç‡ã€‚ä»…åœ¨ `usesLanguageCorrection = true` æ—¶ç”Ÿæ•ˆã€‚

### 6.6 æœ€å°æ–‡å­—é«˜åº¦ (minimumTextHeight)

```swift
request.minimumTextHeight = 0.02  // ç›¸å¯¹äºå›¾åƒé«˜åº¦çš„æ¯”ä¾‹
```

è¿‡æ»¤è¿‡å°çš„æ–‡å­—ï¼Œå‡å°‘å™ªå£°ã€‚

### 6.7 ç‰ˆæœ¬æ§åˆ¶ (revision)

```swift
request.revision = VNRecognizeTextRequestRevision3
```

| ç‰ˆæœ¬ | å¼•å…¥ç³»ç»Ÿ | ç‰¹ç‚¹ |
|-----|---------|------|
| `Revision1` | macOS 10.15 | åŸºç¡€ç‰ˆæœ¬ |
| `Revision2` | macOS 11.0 | æ”¹è¿›ä¸­æ–‡æ”¯æŒ |
| `Revision3` | macOS 14.0 | æœ€æ–°ï¼Œæœ€ä½³æ€§èƒ½ |

---

## 7. åŠŸèƒ½æ¦‚è§ˆ

### 7.1 Apple Vision OCR åŠŸèƒ½

| åŠŸèƒ½ | æ”¯æŒæƒ…å†µ |
|-----|---------|
| ä¸­æ–‡è¯†åˆ«ï¼ˆç®€ä½“/ç¹ä½“ï¼‰ | âœ… å®Œå…¨æ”¯æŒ |
| è‹±æ–‡è¯†åˆ« | âœ… å®Œå…¨æ”¯æŒ |
| æ‰‹å†™è¯†åˆ« | â­â­â­ åŸºç¡€æ”¯æŒ |
| BBox ç²¾åº¦ | â­â­â­â­ è‰¯å¥½ |
| é€Ÿåº¦ï¼ˆApple Siliconï¼‰ | â­â­â­â­â­ æå¿« |
| ç¦»çº¿å¯ç”¨ | âœ… å®Œå…¨ç¦»çº¿ |
| éšç§ä¿æŠ¤ | â­â­â­â­â­ æœ¬åœ°å¤„ç† |

### 7.2 æ€§èƒ½å‚è€ƒï¼ˆApple Silicon M1ï¼‰

| å›¾ç‰‡å°ºå¯¸ | å¤„ç†æ—¶é—´ |
|---------|---------|
| 1080p | ~200-500ms |
| 4K | ~500-1000ms |

### 7.3 SyncNos èŠå¤©æˆªå›¾åœºæ™¯é€‚ç”¨æ€§

| éœ€æ±‚ | Apple Vision æ”¯æŒ |
|-----|-----------------|
| è¯†åˆ«ä¸­æ–‡èŠå¤©å†…å®¹ | âœ… å®Œå…¨æ”¯æŒ |
| è¯†åˆ«è‹±æ–‡æ··åˆå†…å®¹ | âœ… å®Œå…¨æ”¯æŒ |
| è¿”å› BBox | âœ… å®Œå…¨æ”¯æŒï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰ |
| åŒºåˆ†æ°”æ³¡æ–¹å‘ | âœ… é€šè¿‡ BBox ä½ç½®åˆ¤æ–­ |
| ç³»ç»Ÿæ¶ˆæ¯æ£€æµ‹ | âœ… é€šè¿‡ BBox å±…ä¸­åˆ¤æ–­ |
| æ—¶é—´æˆ³æ£€æµ‹ | âœ… é€šè¿‡ BBox ä½ç½®åˆ¤æ–­ |

---

## 8. JSON åŸå§‹æ•°æ®ç¤ºä¾‹

### 8.1 è¯†åˆ«ç»“æœç»“æ„

```json
{
  "observations": [
    {
      "text": "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸå¥½",
      "confidence": 0.98,
      "normalizedBoundingBox": {
        "x": 0.05,
        "y": 0.15,
        "width": 0.35,
        "height": 0.03
      },
      "pixelBoundingBox": {
        "x": 54,
        "y": 873,
        "width": 378,
        "height": 33
      },
      "quadrilateral": {
        "topLeft": { "x": 0.05, "y": 0.18 },
        "topRight": { "x": 0.40, "y": 0.18 },
        "bottomLeft": { "x": 0.05, "y": 0.15 },
        "bottomRight": { "x": 0.40, "y": 0.15 }
      }
    },
    {
      "text": "æ˜¯å•Šï¼Œé€‚åˆå‡ºå»èµ°èµ°",
      "confidence": 0.95,
      "normalizedBoundingBox": {
        "x": 0.55,
        "y": 0.25,
        "width": 0.40,
        "height": 0.03
      },
      "pixelBoundingBox": {
        "x": 594,
        "y": 765,
        "width": 432,
        "height": 33
      }
    }
  ],
  "imageSize": {
    "width": 1080,
    "height": 1920
  }
}
```

### 8.2 OCRBlock ç»“æ„

Vision æ¡†æ¶è¿”å›çš„æ•°æ®æ˜ å°„åˆ° `OCRBlock` ç»“æ„ï¼š

```json
{
  "blocks": [{
    "text": "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸå¥½",
    "label": "text",
    "bbox": { "x": 54, "y": 150, "width": 378, "height": 33 }
  }]
}
```

---

## 9. SyncNos å®ç°

### 9.1 ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶ | æè¿° |
|-----|------|
| `VisionOCRService.swift` | Vision OCR æœåŠ¡å®ç°ï¼ˆå«è¯¦ç»†è¯†åˆ«æ—¥å¿—ï¼‰ |
| `OCRConfigStore.swift` | è¯­è¨€é…ç½®å­˜å‚¨ï¼ˆ`OCRLanguage`ã€30 ç§è¯­è¨€ã€`selectedLanguageCodes`ï¼‰ |
| `OCRModels.swift` | æ•°æ®æ¨¡å‹ï¼ˆ`OCRResult`ã€`OCRBlock`ï¼‰å’Œåè®®å®šä¹‰ |
| `OCRSettingsView.swift` | è®¾ç½®ç•Œé¢ï¼ˆè¯­è¨€é€‰æ‹©ã€Debug æµ‹è¯•ï¼‰ï¼Œä½äº Settings â†’ Chats |
| `DIContainer.swift` | æœåŠ¡æ³¨å†Œ |

### 9.2 OCRAPIServiceProtocol åè®®

```swift
protocol OCRAPIServiceProtocol {
    func recognize(_ image: NSImage) async throws -> OCRResult
    func recognizeWithRaw(_ image: NSImage, config: OCRRequestConfig) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data)
    func testConnection() async throws -> Bool
}
```

`VisionOCRService` å®Œå…¨éµå¾ªæ­¤åè®®ã€‚

---

## 10. é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

### 10.1 å·²çŸ¥é™åˆ¶

1. **ä¸­æ–‡ä¸æ—¥è¯­ä¸èƒ½æ··åˆ**ï¼šå¦‚æœéœ€è¦åŒæ—¶è¯†åˆ«ä¸­æ—¥æ–‡ï¼Œéœ€è¦åˆ†ä¸¤æ¬¡è¯·æ±‚
2. **æ— ç‰ˆé¢åˆ†æ**ï¼šä¸æ”¯æŒè¡¨æ ¼ã€å…¬å¼ç­‰ç»“æ„åŒ–å†…å®¹è¯†åˆ«
3. **æ‰‹å†™ä½“**ï¼šä¸­æ–‡æ‰‹å†™è¯†åˆ«æ•ˆæœä¸€èˆ¬
4. **å€¾æ–œæ–‡æœ¬**ï¼šä¸¥é‡å€¾æ–œçš„æ–‡æœ¬å¯èƒ½è¯†åˆ«å¤±è´¥

### 10.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å¼‚æ­¥å¤„ç†**ï¼šå§‹ç»ˆåœ¨åå°çº¿ç¨‹æ‰§è¡Œ OCR
2. **å›¾ç‰‡é¢„å¤„ç†**ï¼šå‹ç¼©è¿‡å¤§çš„å›¾ç‰‡ï¼ˆ>4Kï¼‰
3. **ç¼“å­˜ç»“æœ**ï¼šç›¸åŒå›¾ç‰‡ä¸é‡å¤è¯†åˆ«
4. **æ‰¹é‡å¤„ç†**ï¼šä½¿ç”¨ `perform([request1, request2, ...])` æ‰¹é‡å¤„ç†

### 10.3 è°ƒè¯•æŠ€å·§

**ä½¿ç”¨å†…ç½® Debug å·¥å…·**

1. æ‰“å¼€ Settings â†’ OCR Settings
2. ç‚¹å‡» "Test OCR Recognition" æŒ‰é’®
3. å¯¼å…¥æµ‹è¯•å›¾ç‰‡ï¼ˆæ”¯æŒæ‹–æ”¾ï¼‰
4. æŸ¥çœ‹è¯†åˆ«ç»“æœã€ç»Ÿè®¡ä¿¡æ¯å’Œå—è¯¦æƒ…

**æŸ¥çœ‹æ—¥å¿—**

æ‰“å¼€ Settings â†’ Logs çª—å£ï¼Œè¿‡æ»¤ `[VisionOCR]` æŸ¥çœ‹è¯†åˆ«æ—¥å¿—ï¼š

```
[VisionOCR] Starting recognition, image size: 1080x1920
[VisionOCR] Language config: Auto (using defaults: zh-Hans, zh-Hant, en-US)
[VisionOCR] âœ… Recognition completed: 25 blocks
[VisionOCR] ğŸ“Š Confidence: avg=0.95, min=0.82, max=0.99
[VisionOCR] ğŸŒ Detected scripts: CJK, Latin
```

**ä»£ç è°ƒè¯•**

```swift
// æ‰“å°æ”¯æŒçš„è¯­è¨€
if let languages = try? VNRecognizeTextRequest.supportedRecognitionLanguages(for: .accurate, revision: VNRecognizeTextRequestRevision3) {
    print("Supported languages: \(languages)")
}

// æ‰“å°è¯†åˆ«ç»“æœè¯¦æƒ…
for observation in observations {
    print("BBox: \(observation.boundingBox)")
    for candidate in observation.topCandidates(3) {
        print("  - \(candidate.string) (\(candidate.confidence))")
    }
}
```

---

## 11. æ€»ç»“

### 11.1 Apple Vision é€‚ç”¨æ€§è¯„ä¼°

| è¯„ä¼°é¡¹ | ç»“è®º |
|-------|------|
| æ»¡è¶³ SyncNos æ ¸å¿ƒéœ€æ±‚ | âœ… |
| ä¸­è‹±æ–‡èŠå¤©è¯†åˆ« | âœ… ä¼˜ç§€ |
| BBox æ•°æ®å®Œæ•´æ€§ | âœ… å®Œå…¨æ”¯æŒ |
| ä¸ç°æœ‰ä»£ç å…¼å®¹ | âœ… æ— éœ€å¤§æ”¹ |
| Mac App Store å…¼å®¹ | âœ… åŸç”Ÿæ”¯æŒ |
| ç”¨æˆ·ä½“éªŒ | âœ… å³è£…å³ç”¨ |

### 11.2 æ¨èæ–¹æ¡ˆ

**é‡‡ç”¨ Apple Vision ä½œä¸º SyncNos é»˜è®¤ OCR å¼•æ“**ï¼š

1. é›¶é…ç½®ï¼Œç”¨æˆ·å³è£…å³ç”¨
2. å®Œå…¨ç¦»çº¿ï¼Œä¿æŠ¤éšç§
3. åˆ©ç”¨ Apple Silicon ä¼˜åŒ–ï¼Œæ€§èƒ½ä¼˜ç§€
4. å®Œå…¨å…¼å®¹ Mac App Store
5. ä¸ç°æœ‰ `ChatOCRParser` æ— ç¼é›†æˆ

SyncNos ä½¿ç”¨ Apple Vision ä½œä¸ºå”¯ä¸€çš„ OCR å¼•æ“ï¼Œæ»¡è¶³èŠå¤©æˆªå›¾è¯†åˆ«çš„æ‰€æœ‰éœ€æ±‚ã€‚

---

## 12. å‚è€ƒèµ„æ–™

### 12.1 Apple å®˜æ–¹æ–‡æ¡£

- [VNRecognizeTextRequest](https://developer.apple.com/documentation/vision/vnrecognizetextrequest/)
- [VNRecognizedTextObservation](https://developer.apple.com/documentation/vision/vnrecognizedtextobservation)
- [Recognizing Text in Images](https://developer.apple.com/documentation/vision/recognizing-text-in-images/)
- [Locating and Displaying Recognized Text](https://developer.apple.com/documentation/vision/locating-and-displaying-recognized-text)

### 12.2 ç¤ºä¾‹é¡¹ç›®

- [Apple Sample Code: Locating and displaying recognized text](https://developer.apple.com/documentation/vision/locating-and-displaying-recognized-text)
- [Apple Sample Code: Extracting phone numbers from text in images](https://developer.apple.com/documentation/vision/extracting-phone-numbers-from-text-in-images)

### 12.3 WWDC è§†é¢‘

- WWDC 2019: [Vision Framework: Understanding Images](https://developer.apple.com/videos/play/wwdc2019/222/)
- WWDC 2021: [Extract document data using Vision](https://developer.apple.com/videos/play/wwdc2021/10041/)
- WWDC 2024: [Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/)

---

*æ–‡æ¡£ç‰ˆæœ¬: 1.0*
*åˆ›å»ºæ—¥æœŸ: 2025-01-29*
*é€‚ç”¨é¡¹ç›®: SyncNos macOS*

