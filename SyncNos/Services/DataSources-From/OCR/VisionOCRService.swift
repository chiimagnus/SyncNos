import Foundation
import AppKit
@preconcurrency import Vision

// MARK: - Vision OCR Service

/// Apple Vision æ¡†æ¶ OCR æœåŠ¡
/// ä½¿ç”¨åŸç”Ÿ VNRecognizeTextRequest è¿›è¡Œæ–‡æœ¬è¯†åˆ«ï¼Œæ— éœ€å¤–éƒ¨ API
/// é€‚ç”¨äº macOS 14.0+ / iOS 17.0+
final class VisionOCRService: OCRAPIServiceProtocol, @unchecked Sendable {
    
    private let logger: LoggerServiceProtocol
    private let configStore: OCRConfigStoreProtocol
    
    // MARK: - Constants
    
    private enum Constants {
        /// æœ€å°æ–‡å­—é«˜åº¦æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒé«˜åº¦ï¼‰
        static let minimumTextHeight: Float = 0.01
        
        /// é•¿å›¾ç‰‡åˆ†ç‰‡å¤„ç†é˜ˆå€¼ï¼ˆåƒç´ ï¼‰
        /// è¶…è¿‡æ­¤é«˜åº¦çš„å›¾ç‰‡ä¼šè¢«åˆ†ç‰‡å¤„ç†ï¼Œé¿å… Vision OCR è¿”å›ç©ºç»“æœ
        /// Apple Silicon Mac æ”¯æŒæœ€å¤§çº¹ç†å°ºå¯¸çº¦ 16384x16384
        /// è®¾ç½®ä¸º 16000pxï¼Œæ¥è¿‘ GPU çº¹ç†é™åˆ¶ä½†ä¿ç•™ä¸€äº›ä½™é‡
        static let sliceThresholdHeight: CGFloat = 16000
        
        /// åˆ†ç‰‡æœ€å¤§é«˜åº¦ï¼ˆåƒç´ ï¼‰
        /// è®¾ç½®ä¸º 8000pxï¼Œç¡®ä¿æ¯ä¸ªåˆ†ç‰‡éƒ½åœ¨ Vision OCR çš„å®‰å…¨å¤„ç†èŒƒå›´å†…
        static let sliceMaxHeight: CGFloat = 8000
        
        /// åˆ†ç‰‡é‡å åŒºåŸŸï¼ˆåƒç´ ï¼‰
        /// ç”¨äºå¤„ç†è·¨ç‰‡æ–‡å­—ï¼Œé¿å…è¾¹ç•Œå¤„æ–‡å­—ä¸¢å¤±æˆ–é‡å¤
        static let sliceOverlap: CGFloat = 200
    }
    
    // MARK: - Init
    
    init(
        logger: LoggerServiceProtocol = DIContainer.shared.loggerService,
        configStore: OCRConfigStoreProtocol = OCRConfigStore.shared
    ) {
        self.logger = logger
        self.configStore = configStore
    }
    
    // MARK: - OCRAPIServiceProtocol
    
    func recognize(_ image: NSImage) async throws -> OCRResult {
        let (result, _, _) = try await recognizeWithRaw(image, config: .default)
        return result
    }
    
    func recognizeWithRaw(
        _ image: NSImage,
        config: OCRRequestConfig
    ) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        guard let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
            throw OCRServiceError.invalidImage
        }
        
        let imageSize = CGSize(
            width: CGFloat(cgImage.width),
            height: CGFloat(cgImage.height)
        )
        
        // è·å–è¯­è¨€é…ç½®
        let languageCodes = configStore.effectiveLanguageCodes
        let isAutoDetect = configStore.isAutoDetectEnabled
        
        logger.info("[VisionOCR] Starting recognition, image size: \(Int(imageSize.width))x\(Int(imageSize.height))")
        logger.debug("[VisionOCR] Language mode: \(isAutoDetect ? "automatic" : "manual"), languages: \(languageCodes.joined(separator: ", "))")
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†ç‰‡å¤„ç†
        if imageSize.height > Constants.sliceThresholdHeight {
            logger.info("[VisionOCR] ğŸ”ª Image height \(Int(imageSize.height))px exceeds threshold \(Int(Constants.sliceThresholdHeight))px, using slice processing")
            return try await recognizeWithSlicing(cgImage: cgImage, imageSize: imageSize, languageCodes: languageCodes, isAutoDetect: isAutoDetect)
        }
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        let request = VNRecognizeTextRequest()
        request.recognitionLevel = .accurate
        request.usesLanguageCorrection = true
        request.minimumTextHeight = Constants.minimumTextHeight
        
        // ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ï¼ˆmacOS 14+ï¼‰
        if #available(macOS 14.0, *) {
            request.revision = VNRecognizeTextRequestRevision3
            // æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨è‡ªåŠ¨è¯­è¨€æ£€æµ‹
            request.automaticallyDetectsLanguage = isAutoDetect
        } else {
            request.revision = VNRecognizeTextRequestRevision2
        }
        
        // è®¾ç½®è¯†åˆ«è¯­è¨€
        request.recognitionLanguages = languageCodes
        
        // æ‰§è¡Œè¯·æ±‚
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        return try await withCheckedThrowingContinuation { continuation in
            DispatchQueue.global(qos: .userInitiated).async { [weak self] in
                guard let self = self else {
                    continuation.resume(throwing: OCRServiceError.invalidResponse)
                    return
                }
                
                do {
                    try handler.perform([request])
                    
                    guard let observations = request.results else {
                        self.logger.warning("[VisionOCR] No observations returned")
                        // è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯ï¼ˆå›¾ç‰‡å¯èƒ½ç¡®å®æ²¡æœ‰æ–‡å­—ï¼‰
                        let emptyResult = OCRResult(
                            rawText: "",
                            markdownText: nil,
                            blocks: [],
                            processedAt: Date(),
                            coordinateSize: imageSize
                        )
                        continuation.resume(returning: (
                            result: emptyResult,
                            rawResponse: Data(),
                            requestJSON: Data()
                        ))
                        return
                    }
                    
                    // è½¬æ¢ä¸º OCRBlock
                    let blocks = self.convertToOCRBlocks(
                        observations: observations,
                        imageSize: imageSize
                    )
                    
                    // æ„é€ ç»“æœ
                    let result = OCRResult(
                        rawText: blocks.map(\.text).joined(separator: "\n"),
                        markdownText: nil,
                        blocks: blocks,
                        processedAt: Date(),
                        coordinateSize: imageSize
                    )
                    
                    // æ„é€  raw responseï¼ˆç”¨äºè°ƒè¯•å’ŒæŒä¹…åŒ–ï¼‰
                    let rawDict = self.observationsToDict(observations, imageSize: imageSize)
                    let rawData = (try? JSONSerialization.data(withJSONObject: rawDict)) ?? Data()
                    
                    // è¯¦ç»†æ—¥å¿—ï¼šæ¯ä¸ªè¯†åˆ«ç»“æœçš„æ–‡æœ¬å’Œç½®ä¿¡åº¦
                    self.logRecognitionDetails(observations: observations, blocks: blocks)
                    
                    continuation.resume(returning: (
                        result: result,
                        rawResponse: rawData,
                        requestJSON: Data()
                    ))
                    
                } catch {
                    self.logger.error("[VisionOCR] Recognition failed: \(error.localizedDescription)")
                    continuation.resume(throwing: error)
                }
            }
        }
    }
    
    func testConnection() async throws -> Bool {
        // Vision æ¡†æ¶å§‹ç»ˆå¯ç”¨ï¼Œæ— éœ€è¿æ¥æµ‹è¯•
        logger.info("[VisionOCR] Connection test: Always available (native framework)")
        return true
    }
    
    // MARK: - Slice Processing (é•¿å›¾ç‰‡åˆ†ç‰‡å¤„ç†)
    
    /// å¯¹è¶…é•¿å›¾ç‰‡è¿›è¡Œåˆ†ç‰‡ OCR å¤„ç†
    /// - Parameters:
    ///   - cgImage: åŸå§‹ CGImage
    ///   - imageSize: å›¾åƒå°ºå¯¸
    ///   - languageCodes: è¯­è¨€ä»£ç 
    ///   - isAutoDetect: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    /// - Returns: åˆå¹¶åçš„ OCR ç»“æœ
    private func recognizeWithSlicing(
        cgImage: CGImage,
        imageSize: CGSize,
        languageCodes: [String],
        isAutoDetect: Bool
    ) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        // è®¡ç®—åˆ†ç‰‡
        let slices = calculateSlices(imageHeight: imageSize.height)
        logger.info("[VisionOCR] ğŸ”ª Slicing image into \(slices.count) parts")
        
        var allBlocks: [OCRBlock] = []
        var allRawDicts: [[String: Any]] = []
        var allObservations: [VNRecognizedTextObservation] = []
        
        for (index, slice) in slices.enumerated() {
            logger.debug("[VisionOCR] ğŸ”ª Processing slice \(index + 1)/\(slices.count): y=\(Int(slice.y)), height=\(Int(slice.height))")
            
            // è£å‰ªå›¾ç‰‡
            guard let slicedImage = cropImage(cgImage, rect: slice, imageWidth: Int(imageSize.width)) else {
                logger.warning("[VisionOCR] âš ï¸ Failed to crop slice \(index + 1)")
                continue
            }
            
            // å¯¹åˆ†ç‰‡è¿›è¡Œ OCR
            let sliceSize = CGSize(width: imageSize.width, height: slice.height)
            let (observations, blocks) = try await recognizeSingleImage(
                cgImage: slicedImage,
                imageSize: sliceSize,
                languageCodes: languageCodes,
                isAutoDetect: isAutoDetect
            )
            
            // è°ƒæ•´ bbox çš„ Y åæ ‡ï¼ˆåŠ ä¸Šåˆ†ç‰‡çš„èµ·å§‹ Y åç§»ï¼‰
            let adjustedBlocks = blocks.map { block -> OCRBlock in
                let adjustedBbox = CGRect(
                    x: block.bbox.origin.x,
                    y: block.bbox.origin.y + slice.y,  // åŠ ä¸Šåˆ†ç‰‡çš„ Y åç§»
                    width: block.bbox.width,
                    height: block.bbox.height
                )
                return OCRBlock(text: block.text, label: block.label, bbox: adjustedBbox)
            }
            
            allBlocks.append(contentsOf: adjustedBlocks)
            allObservations.append(contentsOf: observations)
            
            // æ„é€  raw dictï¼ˆè°ƒæ•´ Y åæ ‡ï¼‰
            let rawDicts = observationsToDict(observations, imageSize: sliceSize).map { dict -> [String: Any] in
                var adjusted = dict
                if var bbox = dict["boundingBox"] as? [String: CGFloat] {
                    bbox["y"] = (bbox["y"] ?? 0) + slice.y
                    adjusted["boundingBox"] = bbox
                }
                return adjusted
            }
            allRawDicts.append(contentsOf: rawDicts)
        }
        
        // å»é‡ï¼šå¤„ç†é‡å åŒºåŸŸå¯èƒ½äº§ç”Ÿçš„é‡å¤æ–‡æœ¬å—
        let deduplicatedBlocks = deduplicateBlocks(allBlocks)
        
        logger.info("[VisionOCR] ğŸ”ª Slice processing completed: \(allBlocks.count) blocks â†’ \(deduplicatedBlocks.count) after deduplication")
        
        // æ„é€ æœ€ç»ˆç»“æœ
        let result = OCRResult(
            rawText: deduplicatedBlocks.map(\.text).joined(separator: "\n"),
            markdownText: nil,
            blocks: deduplicatedBlocks,
            processedAt: Date(),
            coordinateSize: imageSize
        )
        
        let rawData = (try? JSONSerialization.data(withJSONObject: allRawDicts)) ?? Data()
        
        // æ—¥å¿—
        logRecognitionDetails(observations: allObservations, blocks: deduplicatedBlocks)
        
        return (result: result, rawResponse: rawData, requestJSON: Data())
    }
    
    /// è®¡ç®—åˆ†ç‰‡åŒºåŸŸ
    /// - Parameter imageHeight: å›¾åƒé«˜åº¦
    /// - Returns: åˆ†ç‰‡åŒºåŸŸæ•°ç»„ (y, height)
    private func calculateSlices(imageHeight: CGFloat) -> [(y: CGFloat, height: CGFloat)] {
        var slices: [(y: CGFloat, height: CGFloat)] = []
        var currentY: CGFloat = 0
        
        while currentY < imageHeight {
            let remainingHeight = imageHeight - currentY
            let sliceHeight = min(Constants.sliceMaxHeight, remainingHeight)
            slices.append((y: currentY, height: sliceHeight))
            
            // ä¸‹ä¸€ä¸ªåˆ†ç‰‡çš„èµ·å§‹ä½ç½®ï¼ˆå‡å»é‡å åŒºåŸŸï¼‰
            currentY += sliceHeight - Constants.sliceOverlap
            
            // å¦‚æœå‰©ä½™é«˜åº¦å°äºé‡å åŒºåŸŸï¼Œç›´æ¥ç»“æŸ
            if currentY >= imageHeight - Constants.sliceOverlap {
                break
            }
        }
        
        return slices
    }
    
    /// è£å‰ªå›¾ç‰‡
    private func cropImage(_ cgImage: CGImage, rect: (y: CGFloat, height: CGFloat), imageWidth: Int) -> CGImage? {
        let cropRect = CGRect(
            x: 0,
            y: rect.y,
            width: CGFloat(imageWidth),
            height: rect.height
        )
        return cgImage.cropping(to: cropRect)
    }
    
    /// å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œ OCRï¼ˆä¸åˆ†ç‰‡ï¼‰
    private func recognizeSingleImage(
        cgImage: CGImage,
        imageSize: CGSize,
        languageCodes: [String],
        isAutoDetect: Bool
    ) async throws -> (observations: [VNRecognizedTextObservation], blocks: [OCRBlock]) {
        let request = VNRecognizeTextRequest()
        request.recognitionLevel = .accurate
        request.usesLanguageCorrection = true
        request.minimumTextHeight = Constants.minimumTextHeight
        
        if #available(macOS 14.0, *) {
            request.revision = VNRecognizeTextRequestRevision3
            request.automaticallyDetectsLanguage = isAutoDetect
        } else {
            request.revision = VNRecognizeTextRequestRevision2
        }
        
        request.recognitionLanguages = languageCodes
        
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        return try await withCheckedThrowingContinuation { continuation in
            DispatchQueue.global(qos: .userInitiated).async { [weak self] in
                guard let self = self else {
                    continuation.resume(throwing: OCRServiceError.invalidResponse)
                    return
                }
                
                do {
                    try handler.perform([request])
                    
                    let observations = request.results ?? []
                    let blocks = self.convertToOCRBlocks(observations: observations, imageSize: imageSize)
                    
                    continuation.resume(returning: (observations: observations, blocks: blocks))
                } catch {
                    continuation.resume(throwing: error)
                }
            }
        }
    }
    
    /// å»é‡ï¼šç§»é™¤é‡å åŒºåŸŸäº§ç”Ÿçš„é‡å¤æ–‡æœ¬å—
    /// ä½¿ç”¨æ–‡æœ¬å†…å®¹ + è¿‘ä¼¼ä½ç½®åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤
    private func deduplicateBlocks(_ blocks: [OCRBlock]) -> [OCRBlock] {
        var result: [OCRBlock] = []
        
        for block in blocks {
            let isDuplicate = result.contains { existing in
                // æ–‡æœ¬å®Œå…¨ç›¸åŒ
                guard existing.text == block.text else { return false }
                
                // Y åæ ‡æ¥è¿‘ï¼ˆåœ¨é‡å åŒºåŸŸå†…ï¼‰
                let yDifference = abs(existing.bbox.midY - block.bbox.midY)
                guard yDifference < Constants.sliceOverlap else { return false }
                
                // X åæ ‡æ¥è¿‘
                let xDifference = abs(existing.bbox.midX - block.bbox.midX)
                return xDifference < 50  // å…è®¸ 50px çš„æ°´å¹³åå·®
            }
            
            if !isDuplicate {
                result.append(block)
            }
        }
        
        return result
    }
    
    // MARK: - Private Methods
    
    /// å°† VNRecognizedTextObservation è½¬æ¢ä¸º OCRBlock
    private func convertToOCRBlocks(
        observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [OCRBlock] {
        // Vision å½’ä¸€åŒ–åæ ‡ç³»ï¼šåŸç‚¹åœ¨å·¦ä¸‹è§’ï¼ŒY è½´å‘ä¸Š
        // VNImageRectForNormalizedRect åªåšç¼©æ”¾ï¼Œä¸ç¿»è½¬ Y è½´
        // éœ€è¦æ‰‹åŠ¨ç¿»è½¬ Y åæ ‡ä»¥åŒ¹é…å›¾åƒåæ ‡ç³»ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼ŒY è½´å‘ä¸‹ï¼‰
        // è¿™æ ·æ‰èƒ½ä¸æ ‡å‡†å›¾åƒåæ ‡ç³»ä¿æŒä¸€è‡´
        
        let blocks = observations.compactMap { observation -> OCRBlock? in
            guard let topCandidate = observation.topCandidates(1).first else {
                return nil
            }
            
            let text = topCandidate.string.trimmingCharacters(in: .whitespacesAndNewlines)
            guard !text.isEmpty else { return nil }
            
            let normalizedBox = observation.boundingBox
            
            // æ‰‹åŠ¨å°† Vision å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºå›¾åƒåæ ‡ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼‰
            // Y è½´ç¿»è½¬å…¬å¼ï¼šnewY = imageHeight - (normalizedY + normalizedHeight) * imageHeight
            //            = imageHeight * (1 - normalizedY - normalizedHeight)
            let x = normalizedBox.origin.x * imageSize.width
            let y = imageSize.height * (1 - normalizedBox.origin.y - normalizedBox.height)
            let width = normalizedBox.width * imageSize.width
            let height = normalizedBox.height * imageSize.height
            
            let pixelRect = CGRect(x: x, y: y, width: width, height: height)
            
            return OCRBlock(
                text: text,
                label: "text",  // Vision åªè¿”å›æ–‡æœ¬ç±»å‹
                bbox: pixelRect
            )
        }
        
        return blocks
    }
    
    /// å°† observations è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äº rawResponseï¼‰
    private func observationsToDict(
        _ observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [[String: Any]] {
        return observations.compactMap { obs -> [String: Any]? in
            guard let text = obs.topCandidates(1).first else { return nil }
            
            let normalizedBox = obs.boundingBox
            
            // æ‰‹åŠ¨å°† Vision å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºå›¾åƒåæ ‡ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼‰
            let x = normalizedBox.origin.x * imageSize.width
            let y = imageSize.height * (1 - normalizedBox.origin.y - normalizedBox.height)
            let width = normalizedBox.width * imageSize.width
            let height = normalizedBox.height * imageSize.height
            
            return [
                "text": text.string,
                "confidence": text.confidence,
                "boundingBox": [
                    "x": x,
                    "y": y,
                    "width": width,
                    "height": height
                ],
                "normalizedBoundingBox": [
                    "x": obs.boundingBox.origin.x,
                    "y": obs.boundingBox.origin.y,
                    "width": obs.boundingBox.width,
                    "height": obs.boundingBox.height
                ]
            ]
        }
    }
    
    /// è®°å½•è¯†åˆ«è¯¦æƒ…æ—¥å¿—
    private func logRecognitionDetails(
        observations: [VNRecognizedTextObservation],
        blocks: [OCRBlock]
    ) {
        // ç»Ÿè®¡ä¿¡æ¯
        let totalObservations = observations.count
        let validBlocks = blocks.count
        
        // è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        let confidences = observations.compactMap { $0.topCandidates(1).first?.confidence }
        let avgConfidence = confidences.isEmpty ? 0 : confidences.reduce(0, +) / Float(confidences.count)
        let minConfidence = confidences.min() ?? 0
        let maxConfidence = confidences.max() ?? 0
        
        logger.info("[VisionOCR] âœ… Recognition completed: \(validBlocks) blocks (from \(totalObservations) observations)")
        logger.info("[VisionOCR] ğŸ“Š Confidence: avg=\(String(format: "%.2f", avgConfidence)), min=\(String(format: "%.2f", minConfidence)), max=\(String(format: "%.2f", maxConfidence))")
        
        // æ£€æµ‹è¯­è¨€ï¼ˆé€šè¿‡å­—ç¬¦èŒƒå›´ï¼‰
        var detectedScripts: Set<String> = []
        for block in blocks {
            let scripts = detectScripts(in: block.text)
            detectedScripts.formUnion(scripts)
        }
        
        if !detectedScripts.isEmpty {
            logger.info("[VisionOCR] ğŸŒ Detected scripts: \(detectedScripts.sorted().joined(separator: ", "))")
        }
        
        // è¾“å‡ºå‰å‡ ä¸ªè¯†åˆ«ç»“æœï¼ˆè°ƒè¯•ç”¨ï¼‰
        let previewCount = min(5, blocks.count)
        if previewCount > 0 {
            logger.debug("[VisionOCR] ğŸ“ First \(previewCount) blocks:")
            for (index, block) in blocks.prefix(previewCount).enumerated() {
                let truncatedText = block.text.count > 50 
                    ? String(block.text.prefix(50)) + "..." 
                    : block.text
                let conf = observations[safe: index].flatMap { $0.topCandidates(1).first?.confidence } ?? 0
                logger.debug("[VisionOCR]   [\(index + 1)] \"\(truncatedText)\" (conf: \(String(format: "%.2f", conf)))")
            }
        }
    }
    
    /// æ£€æµ‹æ–‡æœ¬ä¸­ä½¿ç”¨çš„ä¹¦å†™ç³»ç»Ÿ
    private func detectScripts(in text: String) -> Set<String> {
        var scripts: Set<String> = []
        
        for scalar in text.unicodeScalars {
            if CharacterSet(charactersIn: "\u{4E00}"..."\u{9FFF}").contains(scalar) ||
               CharacterSet(charactersIn: "\u{3400}"..."\u{4DBF}").contains(scalar) {
                scripts.insert("CJK (Chinese/Japanese Kanji)")
            } else if CharacterSet(charactersIn: "\u{3040}"..."\u{309F}").contains(scalar) {
                scripts.insert("Hiragana (Japanese)")
            } else if CharacterSet(charactersIn: "\u{30A0}"..."\u{30FF}").contains(scalar) {
                scripts.insert("Katakana (Japanese)")
            } else if CharacterSet(charactersIn: "\u{AC00}"..."\u{D7AF}").contains(scalar) ||
                      CharacterSet(charactersIn: "\u{1100}"..."\u{11FF}").contains(scalar) {
                scripts.insert("Hangul (Korean)")
            } else if CharacterSet(charactersIn: "\u{0600}"..."\u{06FF}").contains(scalar) {
                scripts.insert("Arabic")
            } else if CharacterSet(charactersIn: "\u{0400}"..."\u{04FF}").contains(scalar) {
                scripts.insert("Cyrillic (Russian/Ukrainian)")
            } else if CharacterSet(charactersIn: "\u{0E00}"..."\u{0E7F}").contains(scalar) {
                scripts.insert("Thai")
            } else if CharacterSet.letters.contains(scalar) && 
                      CharacterSet(charactersIn: "a"..."z").contains(scalar) ||
                      CharacterSet(charactersIn: "A"..."Z").contains(scalar) {
                scripts.insert("Latin (English/European)")
            }
        }
        
        return scripts
    }
}

// MARK: - Array Safe Subscript

private extension Array {
    subscript(safe index: Index) -> Element? {
        return indices.contains(index) ? self[index] : nil
    }
}

// MARK: - Vision OCR Error

extension VisionOCRService {
    enum VisionOCRError: LocalizedError {
        case recognitionFailed(Error)
        
        var errorDescription: String? {
            switch self {
            case .recognitionFailed(let error):
                return "Vision OCR è¯†åˆ«å¤±è´¥: \(error.localizedDescription)"
            }
        }
    }
}
