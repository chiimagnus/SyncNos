import Foundation
import AppKit
@preconcurrency import Vision

// MARK: - Vision OCR Service

/// Apple Vision æ¡†æž¶ OCR æœåŠ¡
/// ä½¿ç”¨åŽŸç”Ÿ VNRecognizeTextRequest è¿›è¡Œæ–‡æœ¬è¯†åˆ«ï¼Œæ— éœ€å¤–éƒ¨ API
/// é€‚ç”¨äºŽ macOS 14.0+ / iOS 17.0+
final class VisionOCRService: OCRAPIServiceProtocol, @unchecked Sendable {
    
    // MARK: - Dependencies
    
    private let logger: LoggerServiceProtocol
    private let configStore: OCRConfigStoreProtocol
    
    // MARK: - Constants
    
    private enum Constants {
        /// æœ€å°æ–‡å­—é«˜åº¦æ¯”ä¾‹ï¼ˆç›¸å¯¹äºŽå›¾åƒé«˜åº¦ï¼‰
        static let minimumTextHeight: Float = 0.01
        
        /// é•¿å›¾ç‰‡åˆ†ç‰‡å¤„ç†é˜ˆå€¼ï¼ˆåƒç´ ï¼‰
        /// è¶…è¿‡æ­¤é«˜åº¦çš„å›¾ç‰‡ä¼šè¢«åˆ†ç‰‡å¤„ç†ï¼Œé¿å… Vision OCR è¿”å›žç©ºç»“æžœ
        /// Apple Silicon Mac æ”¯æŒæœ€å¤§çº¹ç†å°ºå¯¸çº¦ 16384x16384
        /// è®¾ç½®ä¸º 16000pxï¼ŒæŽ¥è¿‘ GPU çº¹ç†é™åˆ¶ä½†ä¿ç•™ä¸€äº›ä½™é‡
        static let sliceThresholdHeight: CGFloat = 16000
        
        /// åˆ†ç‰‡æœ€å¤§é«˜åº¦ï¼ˆåƒç´ ï¼‰
        /// è®¾ç½®ä¸º 8000pxï¼Œç¡®ä¿æ¯ä¸ªåˆ†ç‰‡éƒ½åœ¨ Vision OCR çš„å®‰å…¨å¤„ç†èŒƒå›´å†…
        static let sliceMaxHeight: CGFloat = 8000
        
        /// åˆ†ç‰‡é‡å åŒºåŸŸï¼ˆåƒç´ ï¼‰
        /// ç”¨äºŽå¤„ç†è·¨ç‰‡æ–‡å­—ï¼Œé¿å…è¾¹ç•Œå¤„æ–‡å­—ä¸¢å¤±æˆ–é‡å¤
        static let sliceOverlap: CGFloat = 200
        
        /// åŽ»é‡æ—¶å…è®¸çš„æ°´å¹³åå·®ï¼ˆåƒç´ ï¼‰
        static let deduplicateXTolerance: CGFloat = 50
    }
    
    // MARK: - Init
    
    init(
        logger: LoggerServiceProtocol = DIContainer.shared.loggerService,
        configStore: OCRConfigStoreProtocol = OCRConfigStore.shared
    ) {
        self.logger = logger
        self.configStore = configStore
    }
    
    // MARK: - OCRAPIServiceProtocol
    
    func recognize(_ image: NSImage) async throws -> OCRResult {
        let (result, _, _) = try await recognizeWithRaw(image, config: .default)
        return result
    }
    
    func recognizeWithRaw(
        _ image: NSImage,
        config: OCRRequestConfig
    ) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        guard let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
            throw OCRServiceError.invalidImage
        }
        
        let imageSize = CGSize(width: CGFloat(cgImage.width), height: CGFloat(cgImage.height))
        let languageCodes = configStore.effectiveLanguageCodes
        let isAutoDetect = configStore.isAutoDetectEnabled
        
        logger.info("[VisionOCR] Starting recognition, image size: \(Int(imageSize.width))x\(Int(imageSize.height))")
        logger.debug("[VisionOCR] Language mode: \(isAutoDetect ? "automatic" : "manual"), languages: \(languageCodes.joined(separator: ", "))")
        
        // æ ¹æ®å›¾åƒé«˜åº¦å†³å®šå¤„ç†æ–¹å¼
        if imageSize.height > Constants.sliceThresholdHeight {
            logger.info("[VisionOCR] ðŸ”ª Image height \(Int(imageSize.height))px exceeds threshold \(Int(Constants.sliceThresholdHeight))px, using slice processing")
            return try await recognizeWithSlicing(
                cgImage: cgImage,
                imageSize: imageSize,
                languageCodes: languageCodes,
                isAutoDetect: isAutoDetect
            )
        }
        
        return try await recognizeStandard(
            cgImage: cgImage,
            imageSize: imageSize,
            languageCodes: languageCodes,
            isAutoDetect: isAutoDetect
        )
    }
    
    func testConnection() async throws -> Bool {
        logger.info("[VisionOCR] Connection test: Always available (native framework)")
        return true
    }
}

// MARK: - Standard Recognition (æ ‡å‡†è¯†åˆ«)

private extension VisionOCRService {
    
    /// æ ‡å‡† OCR è¯†åˆ«ï¼ˆä¸åˆ†ç‰‡ï¼‰
    func recognizeStandard(
        cgImage: CGImage,
        imageSize: CGSize,
        languageCodes: [String],
        isAutoDetect: Bool
    ) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        let request = createTextRecognitionRequest(languageCodes: languageCodes, isAutoDetect: isAutoDetect)
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        return try await withCheckedThrowingContinuation { continuation in
            DispatchQueue.global(qos: .userInitiated).async { [weak self] in
                guard let self else {
                    continuation.resume(throwing: OCRServiceError.invalidResponse)
                    return
                }
                
                do {
                    try handler.perform([request])
                    
                    guard let observations = request.results else {
                        self.logger.warning("[VisionOCR] No observations returned")
                        continuation.resume(returning: self.createEmptyResult(imageSize: imageSize))
                        return
                    }
                    
                    let result = self.buildResult(
                        observations: observations,
                        imageSize: imageSize
                    )
                    continuation.resume(returning: result)
                    
                } catch {
                    self.logger.error("[VisionOCR] Recognition failed: \(error.localizedDescription)")
                    continuation.resume(throwing: error)
                }
            }
        }
    }
    
    /// å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œ OCRï¼ˆç”¨äºŽåˆ†ç‰‡å¤„ç†ï¼‰
    func recognizeSingle(
        cgImage: CGImage,
        imageSize: CGSize,
        languageCodes: [String],
        isAutoDetect: Bool
    ) async throws -> (observations: [VNRecognizedTextObservation], blocks: [OCRBlock]) {
        let request = createTextRecognitionRequest(languageCodes: languageCodes, isAutoDetect: isAutoDetect)
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        return try await withCheckedThrowingContinuation { continuation in
            DispatchQueue.global(qos: .userInitiated).async { [weak self] in
                guard let self else {
                    continuation.resume(throwing: OCRServiceError.invalidResponse)
                    return
                }
                
                do {
                    try handler.perform([request])
                    let observations = request.results ?? []
                    let blocks = self.convertToOCRBlocks(observations: observations, imageSize: imageSize)
                    continuation.resume(returning: (observations, blocks))
                } catch {
                    continuation.resume(throwing: error)
                }
            }
        }
    }
}

// MARK: - Slice Processing (é•¿å›¾ç‰‡åˆ†ç‰‡å¤„ç†)

private extension VisionOCRService {
    
    /// å¯¹è¶…é•¿å›¾ç‰‡è¿›è¡Œåˆ†ç‰‡ OCR å¤„ç†
    func recognizeWithSlicing(
        cgImage: CGImage,
        imageSize: CGSize,
        languageCodes: [String],
        isAutoDetect: Bool
    ) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        let slices = calculateSlices(imageHeight: imageSize.height)
        logger.info("[VisionOCR] ðŸ”ª Slicing image into \(slices.count) parts")
        
        var allBlocks: [OCRBlock] = []
        var allRawDicts: [[String: Any]] = []
        var allObservations: [VNRecognizedTextObservation] = []
        
        for (index, slice) in slices.enumerated() {
            logger.debug("[VisionOCR] ðŸ”ª Processing slice \(index + 1)/\(slices.count): y=\(Int(slice.y)), height=\(Int(slice.height))")
            
            guard let slicedImage = cropImage(cgImage, slice: slice, imageWidth: Int(imageSize.width)) else {
                logger.warning("[VisionOCR] âš ï¸ Failed to crop slice \(index + 1)")
                continue
            }
            
            let sliceSize = CGSize(width: imageSize.width, height: slice.height)
            let (observations, blocks) = try await recognizeSingle(
                cgImage: slicedImage,
                imageSize: sliceSize,
                languageCodes: languageCodes,
                isAutoDetect: isAutoDetect
            )
            
            // è°ƒæ•´ Y åæ ‡å¹¶æ”¶é›†ç»“æžœ
            let adjustedBlocks = adjustBlocksYOffset(blocks, yOffset: slice.y)
            allBlocks.append(contentsOf: adjustedBlocks)
            allObservations.append(contentsOf: observations)
            
            let rawDicts = observationsToDict(observations, imageSize: sliceSize)
                .map { adjustRawDictYOffset($0, yOffset: slice.y) }
            allRawDicts.append(contentsOf: rawDicts)
        }
        
        // åŽ»é‡å¹¶æž„å»ºç»“æžœ
        let deduplicatedBlocks = deduplicateBlocks(allBlocks)
        logger.info("[VisionOCR] ðŸ”ª Slice processing completed: \(allBlocks.count) blocks â†’ \(deduplicatedBlocks.count) after deduplication")
        
        let result = OCRResult(
            rawText: deduplicatedBlocks.map(\.text).joined(separator: "\n"),
            markdownText: nil,
            blocks: deduplicatedBlocks,
            processedAt: Date(),
            coordinateSize: imageSize
        )
        
        let rawData = (try? JSONSerialization.data(withJSONObject: allRawDicts)) ?? Data()
        logRecognitionDetails(observations: allObservations, blocks: deduplicatedBlocks)
        
        return (result: result, rawResponse: rawData, requestJSON: Data())
    }
    
    /// è®¡ç®—åˆ†ç‰‡åŒºåŸŸ
    func calculateSlices(imageHeight: CGFloat) -> [(y: CGFloat, height: CGFloat)] {
        var slices: [(y: CGFloat, height: CGFloat)] = []
        var currentY: CGFloat = 0
        
        while currentY < imageHeight {
            let remainingHeight = imageHeight - currentY
            let sliceHeight = min(Constants.sliceMaxHeight, remainingHeight)
            slices.append((y: currentY, height: sliceHeight))
            
            currentY += sliceHeight - Constants.sliceOverlap
            
            if currentY >= imageHeight - Constants.sliceOverlap {
                break
            }
        }
        
        return slices
    }
    
    /// è£å‰ªå›¾ç‰‡
    func cropImage(_ cgImage: CGImage, slice: (y: CGFloat, height: CGFloat), imageWidth: Int) -> CGImage? {
        let cropRect = CGRect(x: 0, y: slice.y, width: CGFloat(imageWidth), height: slice.height)
        return cgImage.cropping(to: cropRect)
    }
    
    /// è°ƒæ•´ blocks çš„ Y åæ ‡åç§»
    func adjustBlocksYOffset(_ blocks: [OCRBlock], yOffset: CGFloat) -> [OCRBlock] {
        blocks.map { block in
            let adjustedBbox = CGRect(
                x: block.bbox.origin.x,
                y: block.bbox.origin.y + yOffset,
                width: block.bbox.width,
                height: block.bbox.height
            )
            return OCRBlock(text: block.text, label: block.label, bbox: adjustedBbox)
        }
    }
    
    /// è°ƒæ•´ raw dict çš„ Y åæ ‡åç§»
    func adjustRawDictYOffset(_ dict: [String: Any], yOffset: CGFloat) -> [String: Any] {
        var adjusted = dict
        if var bbox = dict["boundingBox"] as? [String: CGFloat] {
            bbox["y"] = (bbox["y"] ?? 0) + yOffset
            adjusted["boundingBox"] = bbox
        }
        return adjusted
    }
    
    /// åŽ»é‡ï¼šç§»é™¤é‡å åŒºåŸŸäº§ç”Ÿçš„é‡å¤æ–‡æœ¬å—
    func deduplicateBlocks(_ blocks: [OCRBlock]) -> [OCRBlock] {
        var result: [OCRBlock] = []
        
        for block in blocks {
            let isDuplicate = result.contains { existing in
                guard existing.text == block.text else { return false }
                
                let yDifference = abs(existing.bbox.midY - block.bbox.midY)
                guard yDifference < Constants.sliceOverlap else { return false }
                
                let xDifference = abs(existing.bbox.midX - block.bbox.midX)
                return xDifference < Constants.deduplicateXTolerance
            }
            
            if !isDuplicate {
                result.append(block)
            }
        }
        
        return result
    }
}

// MARK: - Request & Result Building (è¯·æ±‚ä¸Žç»“æžœæž„å»º)

private extension VisionOCRService {
    
    /// åˆ›å»ºæ–‡æœ¬è¯†åˆ«è¯·æ±‚
    func createTextRecognitionRequest(languageCodes: [String], isAutoDetect: Bool) -> VNRecognizeTextRequest {
        let request = VNRecognizeTextRequest()
        request.recognitionLevel = .accurate
        request.usesLanguageCorrection = true
        request.minimumTextHeight = Constants.minimumTextHeight
        request.recognitionLanguages = languageCodes
        
        if #available(macOS 14.0, *) {
            request.revision = VNRecognizeTextRequestRevision3
            request.automaticallyDetectsLanguage = isAutoDetect
        } else {
            request.revision = VNRecognizeTextRequestRevision2
        }
        
        return request
    }
    
    /// æž„å»º OCR ç»“æžœ
    func buildResult(
        observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        let blocks = convertToOCRBlocks(observations: observations, imageSize: imageSize)
        
        let result = OCRResult(
            rawText: blocks.map(\.text).joined(separator: "\n"),
            markdownText: nil,
            blocks: blocks,
            processedAt: Date(),
            coordinateSize: imageSize
        )
        
        let rawDict = observationsToDict(observations, imageSize: imageSize)
        let rawData = (try? JSONSerialization.data(withJSONObject: rawDict)) ?? Data()
        
        logRecognitionDetails(observations: observations, blocks: blocks)
        
        return (result: result, rawResponse: rawData, requestJSON: Data())
    }
    
    /// åˆ›å»ºç©ºç»“æžœ
    func createEmptyResult(imageSize: CGSize) -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        let emptyResult = OCRResult(
            rawText: "",
            markdownText: nil,
            blocks: [],
            processedAt: Date(),
            coordinateSize: imageSize
        )
        return (result: emptyResult, rawResponse: Data(), requestJSON: Data())
    }
}

// MARK: - Coordinate Conversion (åæ ‡è½¬æ¢)

private extension VisionOCRService {
    
    /// å°† VNRecognizedTextObservation è½¬æ¢ä¸º OCRBlock
    /// Vision å½’ä¸€åŒ–åæ ‡ç³»ï¼šåŽŸç‚¹åœ¨å·¦ä¸‹è§’ï¼ŒY è½´å‘ä¸Š
    /// éœ€è¦æ‰‹åŠ¨ç¿»è½¬ Y åæ ‡ä»¥åŒ¹é…å›¾åƒåæ ‡ç³»ï¼ˆåŽŸç‚¹å·¦ä¸Šè§’ï¼ŒY è½´å‘ä¸‹ï¼‰
    func convertToOCRBlocks(
        observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [OCRBlock] {
        observations.compactMap { observation -> OCRBlock? in
            guard let topCandidate = observation.topCandidates(1).first else { return nil }
            
            let text = topCandidate.string.trimmingCharacters(in: .whitespacesAndNewlines)
            guard !text.isEmpty else { return nil }
            
            let pixelRect = convertNormalizedToPixelRect(observation.boundingBox, imageSize: imageSize)
            return OCRBlock(text: text, label: "text", bbox: pixelRect)
        }
    }
    
    /// å°† observations è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºŽ rawResponseï¼‰
    func observationsToDict(
        _ observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [[String: Any]] {
        observations.compactMap { obs -> [String: Any]? in
            guard let text = obs.topCandidates(1).first else { return nil }
            
            let pixelRect = convertNormalizedToPixelRect(obs.boundingBox, imageSize: imageSize)
            
            return [
                "text": text.string,
                "confidence": text.confidence,
                "boundingBox": [
                    "x": pixelRect.origin.x,
                    "y": pixelRect.origin.y,
                    "width": pixelRect.width,
                    "height": pixelRect.height
                ],
                "normalizedBoundingBox": [
                    "x": obs.boundingBox.origin.x,
                    "y": obs.boundingBox.origin.y,
                    "width": obs.boundingBox.width,
                    "height": obs.boundingBox.height
                ]
            ]
        }
    }
    
    /// å°† Vision å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡ï¼ˆåŽŸç‚¹å·¦ä¸Šè§’ï¼‰
    func convertNormalizedToPixelRect(_ normalizedBox: CGRect, imageSize: CGSize) -> CGRect {
        let x = normalizedBox.origin.x * imageSize.width
        let y = imageSize.height * (1 - normalizedBox.origin.y - normalizedBox.height)
        let width = normalizedBox.width * imageSize.width
        let height = normalizedBox.height * imageSize.height
        return CGRect(x: x, y: y, width: width, height: height)
    }
}

// MARK: - Logging & Script Detection (æ—¥å¿—ä¸Žè¯­è¨€æ£€æµ‹)

private extension VisionOCRService {
    
    /// è®°å½•è¯†åˆ«è¯¦æƒ…æ—¥å¿—
    func logRecognitionDetails(
        observations: [VNRecognizedTextObservation],
        blocks: [OCRBlock]
    ) {
        let totalObservations = observations.count
        let validBlocks = blocks.count
        
        let confidences = observations.compactMap { $0.topCandidates(1).first?.confidence }
        let avgConfidence = confidences.isEmpty ? 0 : confidences.reduce(0, +) / Float(confidences.count)
        let minConfidence = confidences.min() ?? 0
        let maxConfidence = confidences.max() ?? 0
        
        logger.info("[VisionOCR] âœ… Recognition completed: \(validBlocks) blocks (from \(totalObservations) observations)")
        logger.info("[VisionOCR] ðŸ“Š Confidence: avg=\(String(format: "%.2f", avgConfidence)), min=\(String(format: "%.2f", minConfidence)), max=\(String(format: "%.2f", maxConfidence))")
        
        let detectedScripts = blocks.reduce(into: Set<String>()) { result, block in
            result.formUnion(detectScripts(in: block.text))
        }
        
        if !detectedScripts.isEmpty {
            logger.info("[VisionOCR] ðŸŒ Detected scripts: \(detectedScripts.sorted().joined(separator: ", "))")
        }
        
        logBlockPreview(blocks: blocks, observations: observations)
    }
    
    /// è¾“å‡ºå‰å‡ ä¸ªè¯†åˆ«ç»“æžœé¢„è§ˆ
    func logBlockPreview(blocks: [OCRBlock], observations: [VNRecognizedTextObservation]) {
        let previewCount = min(5, blocks.count)
        guard previewCount > 0 else { return }
        
        logger.debug("[VisionOCR] ðŸ“ First \(previewCount) blocks:")
        for (index, block) in blocks.prefix(previewCount).enumerated() {
            let truncatedText = block.text.count > 50
                ? String(block.text.prefix(50)) + "..."
                : block.text
            let conf = observations[safe: index].flatMap { $0.topCandidates(1).first?.confidence } ?? 0
            logger.debug("[VisionOCR]   [\(index + 1)] \"\(truncatedText)\" (conf: \(String(format: "%.2f", conf)))")
        }
    }
    
    /// æ£€æµ‹æ–‡æœ¬ä¸­ä½¿ç”¨çš„ä¹¦å†™ç³»ç»Ÿ
    func detectScripts(in text: String) -> Set<String> {
        var scripts: Set<String> = []
        
        for scalar in text.unicodeScalars {
            if let script = detectScript(for: scalar) {
                scripts.insert(script)
            }
        }
        
        return scripts
    }
    
    /// æ£€æµ‹å•ä¸ªå­—ç¬¦çš„ä¹¦å†™ç³»ç»Ÿ
    func detectScript(for scalar: Unicode.Scalar) -> String? {
        switch scalar.value {
        case 0x4E00...0x9FFF, 0x3400...0x4DBF:
            return "CJK (Chinese/Japanese Kanji)"
        case 0x3040...0x309F:
            return "Hiragana (Japanese)"
        case 0x30A0...0x30FF:
            return "Katakana (Japanese)"
        case 0xAC00...0xD7AF, 0x1100...0x11FF:
            return "Hangul (Korean)"
        case 0x0600...0x06FF:
            return "Arabic"
        case 0x0400...0x04FF:
            return "Cyrillic (Russian/Ukrainian)"
        case 0x0E00...0x0E7F:
            return "Thai"
        case 0x0041...0x005A, 0x0061...0x007A:
            return "Latin (English/European)"
        default:
            return nil
        }
    }
}

// MARK: - Array Safe Subscript

private extension Array {
    subscript(safe index: Index) -> Element? {
        indices.contains(index) ? self[index] : nil
    }
}

// MARK: - Vision OCR Error

extension VisionOCRService {
    enum VisionOCRError: LocalizedError {
        case recognitionFailed(Error)
        
        var errorDescription: String? {
            switch self {
            case .recognitionFailed(let error):
                return "Vision OCR è¯†åˆ«å¤±è´¥: \(error.localizedDescription)"
            }
        }
    }
}
