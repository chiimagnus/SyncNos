import Foundation
import AppKit
@preconcurrency import Vision

// MARK: - Vision OCR Service

/// Apple Vision æ¡†æ¶ OCR æœåŠ¡
/// ä½¿ç”¨åŸç”Ÿ VNRecognizeTextRequest è¿›è¡Œæ–‡æœ¬è¯†åˆ«ï¼Œæ— éœ€å¤–éƒ¨ API
/// é€‚ç”¨äº macOS 14.0+ / iOS 17.0+
final class VisionOCRService: OCRAPIServiceProtocol, @unchecked Sendable {
    
    private let logger: LoggerServiceProtocol
    private let configStore: OCRConfigStoreProtocol
    
    // MARK: - Constants
    
    private enum Constants {
        /// æœ€å°æ–‡å­—é«˜åº¦æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒé«˜åº¦ï¼‰
        static let minimumTextHeight: Float = 0.01
    }
    
    // MARK: - Init
    
    init(
        logger: LoggerServiceProtocol = DIContainer.shared.loggerService,
        configStore: OCRConfigStoreProtocol = OCRConfigStore.shared
    ) {
        self.logger = logger
        self.configStore = configStore
    }
    
    // MARK: - OCRAPIServiceProtocol
    
    func recognize(_ image: NSImage) async throws -> OCRResult {
        let (result, _, _) = try await recognizeWithRaw(image, config: .default)
        return result
    }
    
    func recognizeWithRaw(
        _ image: NSImage,
        config: OCRRequestConfig
    ) async throws -> (result: OCRResult, rawResponse: Data, requestJSON: Data) {
        guard let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
            throw OCRServiceError.invalidImage
        }
        
        let imageSize = CGSize(
            width: CGFloat(cgImage.width),
            height: CGFloat(cgImage.height)
        )
        
        // è·å–è¯­è¨€é…ç½®
        let languageCodes = configStore.effectiveLanguageCodes
        let isAutoDetect = configStore.isAutoDetectEnabled
        
        logger.info("[VisionOCR] Starting recognition, image size: \(Int(imageSize.width))x\(Int(imageSize.height))")
        logger.debug("[VisionOCR] Language mode: \(isAutoDetect ? "automatic" : "manual"), languages: \(languageCodes.joined(separator: ", "))")
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        let request = VNRecognizeTextRequest()
        request.recognitionLevel = .accurate
        request.usesLanguageCorrection = true
        request.minimumTextHeight = Constants.minimumTextHeight
        
        // ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ï¼ˆmacOS 14+ï¼‰
        if #available(macOS 14.0, *) {
            request.revision = VNRecognizeTextRequestRevision3
            // æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨è‡ªåŠ¨è¯­è¨€æ£€æµ‹
            request.automaticallyDetectsLanguage = isAutoDetect
        } else {
            request.revision = VNRecognizeTextRequestRevision2
        }
        
        // è®¾ç½®è¯†åˆ«è¯­è¨€
        request.recognitionLanguages = languageCodes
        
        // æ‰§è¡Œè¯·æ±‚
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        return try await withCheckedThrowingContinuation { continuation in
            DispatchQueue.global(qos: .userInitiated).async { [weak self] in
                guard let self = self else {
                    continuation.resume(throwing: OCRServiceError.invalidResponse)
                    return
                }
                
                do {
                    try handler.perform([request])
                    
                    guard let observations = request.results else {
                        self.logger.warning("[VisionOCR] No observations returned")
                        // è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯ï¼ˆå›¾ç‰‡å¯èƒ½ç¡®å®æ²¡æœ‰æ–‡å­—ï¼‰
                        let emptyResult = OCRResult(
                            rawText: "",
                            markdownText: nil,
                            blocks: [],
                            processedAt: Date(),
                            coordinateSize: imageSize
                        )
                        continuation.resume(returning: (
                            result: emptyResult,
                            rawResponse: Data(),
                            requestJSON: Data()
                        ))
                        return
                    }
                    
                    // è½¬æ¢ä¸º OCRBlock
                    let blocks = self.convertToOCRBlocks(
                        observations: observations,
                        imageSize: imageSize
                    )
                    
                    // æ„é€ ç»“æœ
                    let result = OCRResult(
                        rawText: blocks.map(\.text).joined(separator: "\n"),
                        markdownText: nil,
                        blocks: blocks,
                        processedAt: Date(),
                        coordinateSize: imageSize
                    )
                    
                    // æ„é€  raw responseï¼ˆç”¨äºè°ƒè¯•å’ŒæŒä¹…åŒ–ï¼‰
                    let rawDict = self.observationsToDict(observations, imageSize: imageSize)
                    let rawData = (try? JSONSerialization.data(withJSONObject: rawDict)) ?? Data()
                    
                    // è¯¦ç»†æ—¥å¿—ï¼šæ¯ä¸ªè¯†åˆ«ç»“æœçš„æ–‡æœ¬å’Œç½®ä¿¡åº¦
                    self.logRecognitionDetails(observations: observations, blocks: blocks)
                    
                    continuation.resume(returning: (
                        result: result,
                        rawResponse: rawData,
                        requestJSON: Data()
                    ))
                    
                } catch {
                    self.logger.error("[VisionOCR] Recognition failed: \(error.localizedDescription)")
                    continuation.resume(throwing: error)
                }
            }
        }
    }
    
    func testConnection() async throws -> Bool {
        // Vision æ¡†æ¶å§‹ç»ˆå¯ç”¨ï¼Œæ— éœ€è¿æ¥æµ‹è¯•
        logger.info("[VisionOCR] Connection test: Always available (native framework)")
        return true
    }
    
    // MARK: - Private Methods
    
    /// å°† VNRecognizedTextObservation è½¬æ¢ä¸º OCRBlock
    private func convertToOCRBlocks(
        observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [OCRBlock] {
        // Vision å½’ä¸€åŒ–åæ ‡ç³»ï¼šåŸç‚¹åœ¨å·¦ä¸‹è§’ï¼ŒY è½´å‘ä¸Š
        // VNImageRectForNormalizedRect åªåšç¼©æ”¾ï¼Œä¸ç¿»è½¬ Y è½´
        // éœ€è¦æ‰‹åŠ¨ç¿»è½¬ Y åæ ‡ä»¥åŒ¹é…å›¾åƒåæ ‡ç³»ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼ŒY è½´å‘ä¸‹ï¼‰
        // è¿™æ ·æ‰èƒ½ä¸æ ‡å‡†å›¾åƒåæ ‡ç³»ä¿æŒä¸€è‡´
        
        let blocks = observations.compactMap { observation -> OCRBlock? in
            guard let topCandidate = observation.topCandidates(1).first else {
                return nil
            }
            
            let text = topCandidate.string.trimmingCharacters(in: .whitespacesAndNewlines)
            guard !text.isEmpty else { return nil }
            
            let normalizedBox = observation.boundingBox
            
            // æ‰‹åŠ¨å°† Vision å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºå›¾åƒåæ ‡ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼‰
            // Y è½´ç¿»è½¬å…¬å¼ï¼šnewY = imageHeight - (normalizedY + normalizedHeight) * imageHeight
            //            = imageHeight * (1 - normalizedY - normalizedHeight)
            let x = normalizedBox.origin.x * imageSize.width
            let y = imageSize.height * (1 - normalizedBox.origin.y - normalizedBox.height)
            let width = normalizedBox.width * imageSize.width
            let height = normalizedBox.height * imageSize.height
            
            let pixelRect = CGRect(x: x, y: y, width: width, height: height)
            
            return OCRBlock(
                text: text,
                label: "text",  // Vision åªè¿”å›æ–‡æœ¬ç±»å‹
                bbox: pixelRect
            )
        }
        
        return blocks
    }
    
    /// å°† observations è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äº rawResponseï¼‰
    private func observationsToDict(
        _ observations: [VNRecognizedTextObservation],
        imageSize: CGSize
    ) -> [[String: Any]] {
        return observations.compactMap { obs -> [String: Any]? in
            guard let text = obs.topCandidates(1).first else { return nil }
            
            let normalizedBox = obs.boundingBox
            
            // æ‰‹åŠ¨å°† Vision å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºå›¾åƒåæ ‡ï¼ˆåŸç‚¹å·¦ä¸Šè§’ï¼‰
            let x = normalizedBox.origin.x * imageSize.width
            let y = imageSize.height * (1 - normalizedBox.origin.y - normalizedBox.height)
            let width = normalizedBox.width * imageSize.width
            let height = normalizedBox.height * imageSize.height
            
            return [
                "text": text.string,
                "confidence": text.confidence,
                "boundingBox": [
                    "x": x,
                    "y": y,
                    "width": width,
                    "height": height
                ],
                "normalizedBoundingBox": [
                    "x": obs.boundingBox.origin.x,
                    "y": obs.boundingBox.origin.y,
                    "width": obs.boundingBox.width,
                    "height": obs.boundingBox.height
                ]
            ]
        }
    }
    
    /// è®°å½•è¯†åˆ«è¯¦æƒ…æ—¥å¿—
    private func logRecognitionDetails(
        observations: [VNRecognizedTextObservation],
        blocks: [OCRBlock]
    ) {
        // ç»Ÿè®¡ä¿¡æ¯
        let totalObservations = observations.count
        let validBlocks = blocks.count
        
        // è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        let confidences = observations.compactMap { $0.topCandidates(1).first?.confidence }
        let avgConfidence = confidences.isEmpty ? 0 : confidences.reduce(0, +) / Float(confidences.count)
        let minConfidence = confidences.min() ?? 0
        let maxConfidence = confidences.max() ?? 0
        
        logger.info("[VisionOCR] âœ… Recognition completed: \(validBlocks) blocks (from \(totalObservations) observations)")
        logger.info("[VisionOCR] ğŸ“Š Confidence: avg=\(String(format: "%.2f", avgConfidence)), min=\(String(format: "%.2f", minConfidence)), max=\(String(format: "%.2f", maxConfidence))")
        
        // æ£€æµ‹è¯­è¨€ï¼ˆé€šè¿‡å­—ç¬¦èŒƒå›´ï¼‰
        var detectedScripts: Set<String> = []
        for block in blocks {
            let scripts = detectScripts(in: block.text)
            detectedScripts.formUnion(scripts)
        }
        
        if !detectedScripts.isEmpty {
            logger.info("[VisionOCR] ğŸŒ Detected scripts: \(detectedScripts.sorted().joined(separator: ", "))")
        }
        
        // è¾“å‡ºå‰å‡ ä¸ªè¯†åˆ«ç»“æœï¼ˆè°ƒè¯•ç”¨ï¼‰
        let previewCount = min(5, blocks.count)
        if previewCount > 0 {
            logger.debug("[VisionOCR] ğŸ“ First \(previewCount) blocks:")
            for (index, block) in blocks.prefix(previewCount).enumerated() {
                let truncatedText = block.text.count > 50 
                    ? String(block.text.prefix(50)) + "..." 
                    : block.text
                let conf = observations[safe: index].flatMap { $0.topCandidates(1).first?.confidence } ?? 0
                logger.debug("[VisionOCR]   [\(index + 1)] \"\(truncatedText)\" (conf: \(String(format: "%.2f", conf)))")
            }
        }
    }
    
    /// æ£€æµ‹æ–‡æœ¬ä¸­ä½¿ç”¨çš„ä¹¦å†™ç³»ç»Ÿ
    private func detectScripts(in text: String) -> Set<String> {
        var scripts: Set<String> = []
        
        for scalar in text.unicodeScalars {
            if CharacterSet(charactersIn: "\u{4E00}"..."\u{9FFF}").contains(scalar) ||
               CharacterSet(charactersIn: "\u{3400}"..."\u{4DBF}").contains(scalar) {
                scripts.insert("CJK (Chinese/Japanese Kanji)")
            } else if CharacterSet(charactersIn: "\u{3040}"..."\u{309F}").contains(scalar) {
                scripts.insert("Hiragana (Japanese)")
            } else if CharacterSet(charactersIn: "\u{30A0}"..."\u{30FF}").contains(scalar) {
                scripts.insert("Katakana (Japanese)")
            } else if CharacterSet(charactersIn: "\u{AC00}"..."\u{D7AF}").contains(scalar) ||
                      CharacterSet(charactersIn: "\u{1100}"..."\u{11FF}").contains(scalar) {
                scripts.insert("Hangul (Korean)")
            } else if CharacterSet(charactersIn: "\u{0600}"..."\u{06FF}").contains(scalar) {
                scripts.insert("Arabic")
            } else if CharacterSet(charactersIn: "\u{0400}"..."\u{04FF}").contains(scalar) {
                scripts.insert("Cyrillic (Russian/Ukrainian)")
            } else if CharacterSet(charactersIn: "\u{0E00}"..."\u{0E7F}").contains(scalar) {
                scripts.insert("Thai")
            } else if CharacterSet.letters.contains(scalar) && 
                      CharacterSet(charactersIn: "a"..."z").contains(scalar) ||
                      CharacterSet(charactersIn: "A"..."Z").contains(scalar) {
                scripts.insert("Latin (English/European)")
            }
        }
        
        return scripts
    }
}

// MARK: - Array Safe Subscript

private extension Array {
    subscript(safe index: Index) -> Element? {
        return indices.contains(index) ? self[index] : nil
    }
}

// MARK: - Vision OCR Error

extension VisionOCRService {
    enum VisionOCRError: LocalizedError {
        case recognitionFailed(Error)
        
        var errorDescription: String? {
            switch self {
            case .recognitionFailed(let error):
                return "Vision OCR è¯†åˆ«å¤±è´¥: \(error.localizedDescription)"
            }
        }
    }
}
